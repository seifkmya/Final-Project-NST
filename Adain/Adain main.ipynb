{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae4cc75-faa6-456e-a3d0-e69996521d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]\n",
      "Executable: /venv/main/bin/python\n",
      "Platform: Linux-6.8.0-60-generic-x86_64-with-glibc2.39\n",
      "Conda env: None\n",
      "Has nvidia-smi? True\n",
      "Thu Aug 21 01:52:35 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5060 Ti     On  |   00000000:03:00.0 Off |                  N/A |\n",
      "|  0%   38C    P8              7W /  180W |       0MiB /  16311MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os, platform, subprocess, shutil\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Conda env:\", os.environ.get(\"CONDA_DEFAULT_ENV\"))\n",
    "print(\"Has nvidia-smi?\", shutil.which(\"nvidia-smi\") is not None)\n",
    "if shutil.which(\"nvidia-smi\"):\n",
    "    print(subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1db26c4-be0c-4699-90fa-d1387b68aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- cell 0: setup ---\n",
    "import os, math, random, time, json, glob\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models, utils\n",
    "\n",
    "# Force CPU for now so everything runs\n",
    "USE_CPU_FOR_NOW = True\n",
    "\n",
    "import torch\n",
    "device = torch.device('cpu' if USE_CPU_FOR_NOW else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3335267-2d18-4173-b302-0d70ab7568f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: /venv/main/bin/python\n",
      "['Thu Aug 21 01:52:50 2025       ', '+-----------------------------------------------------------------------------------------+', '| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |', '|-----------------------------------------+------------------------+----------------------+', '| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |']\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "print(\"Python exe:\", sys.executable)\n",
    "print(subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout.splitlines()[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c310b6b9-cdda-4864-9a35-f50442f1d611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.8.0+cu128\n",
      "Uninstalling torch-2.8.0+cu128:\n",
      "  Successfully uninstalled torch-2.8.0+cu128\n",
      "Found existing installation: torchvision 0.23.0+cu128\n",
      "Uninstalling torchvision-0.23.0+cu128:\n",
      "  Successfully uninstalled torchvision-0.23.0+cu128\n",
      "Found existing installation: torchaudio 2.8.0+cu128\n",
      "Uninstalling torchaudio-2.8.0+cu128:\n",
      "  Successfully uninstalled torchaudio-2.8.0+cu128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /venv/main/lib/python3.12/site-packages (25.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu128/torch-2.8.0%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu128/torchvision-0.23.0%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu128/torchaudio-2.8.0%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.12/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /venv/main/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /venv/main/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /venv/main/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /venv/main/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /venv/main/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /venv/main/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /venv/main/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /venv/main/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /venv/main/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /venv/main/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading https://download.pytorch.org/whl/cu128/torch-2.8.0%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (889.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m889.0/889.0 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached https://download.pytorch.org/whl/cu128/torchvision-0.23.0%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/torchaudio-2.8.0%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (3.9 MB)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [torchaudio]3\u001b[0m [torchaudio]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed torch-2.8.0+cu128 torchaudio-2.8.0+cu128 torchvision-0.23.0+cu128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "# Remove mismatched installs\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"torch\", \"torchvision\", \"torchaudio\"])\n",
    "\n",
    "# Install the CUDA 12.8 build (stable)\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
    "                      \"torch\", \"torchvision\", \"torchaudio\",\n",
    "                      \"--index-url\", \"https://download.pytorch.org/whl/cu128\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b334c5c-a037-438e-ab2c-5e54525892d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# --- cell 1: config ---\n",
    "class Cfg:\n",
    "    # paths\n",
    "    data_root = str((Path.cwd() / \"/workspace\").resolve())\n",
    "    content_dir = os.path.join(data_root, \"content\")   \n",
    "    style_dir   = os.path.join(data_root, \"style\")     \n",
    "    \n",
    "    out_dir     = \"./adain_runs\"                # where to save checkpoints & samples\n",
    "    # os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # training\n",
    "    image_size_crop = 256        # 256x256 training crops (per AdaIN paper)\n",
    "    resize_shorter_to = 512      # resize shorter side -> 512, then random crop 256\n",
    "    batch_size = 8               # adjust to your GPU\n",
    "    num_workers = 2\n",
    "    lr = 1e-4                    # Adam LR used commonly for AdaIN decoder training\n",
    "    max_iterations = 160_000     # total training iterations (common range: 80k-160k+)\n",
    "    save_every = 2000            # save checkpoint every N iterations (your requirement)\n",
    "    log_every = 200              # print losses every N iterations\n",
    "\n",
    "    # loss weights (L = Lc + lambda_style * Ls), paper uses IN-statistics style loss\n",
    "    lambda_style = 10.0\n",
    "\n",
    "    # resume (set to checkpoint path if resuming)\n",
    "    resume = None\n",
    "\n",
    "cfg = Cfg()\n",
    "print(cfg.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5fce23-1648-4a57-a3be-9fcd9252b98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device OK? -> cuda\n",
      "data_root exists: True\n",
      "content_dir exists: True\n",
      "style_dir exists: True\n",
      "content images: 49981\n",
      "style images: 49981\n",
      "example content: /workspace/content/000000000045.jpg\n",
      "example style: /workspace/style/1.jpg\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for Step 1 (Config)\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"Device OK? ->\", device)\n",
    "\n",
    "print(\"data_root exists:\", os.path.isdir(cfg.data_root))\n",
    "print(\"content_dir exists:\", os.path.isdir(cfg.content_dir))\n",
    "print(\"style_dir exists:\", os.path.isdir(cfg.style_dir))\n",
    "\n",
    "IMG_EXTS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')\n",
    "\n",
    "def count_imgs(p):\n",
    "    p = Path(p)\n",
    "    return sum(1 for f in p.rglob(\"*\") if f.suffix.lower() in IMG_EXTS)\n",
    "\n",
    "def first_img(p):\n",
    "    p = Path(p)\n",
    "    for f in p.rglob(\"*\"):\n",
    "        if f.suffix.lower() in IMG_EXTS:\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "n_content = count_imgs(cfg.content_dir) if os.path.isdir(cfg.content_dir) else 0\n",
    "n_style   = count_imgs(cfg.style_dir)   if os.path.isdir(cfg.style_dir)   else 0\n",
    "\n",
    "print(\"content images:\", n_content)\n",
    "print(\"style images:\", n_style)\n",
    "print(\"example content:\", first_img(cfg.content_dir))\n",
    "print(\"example style:\", first_img(cfg.style_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d55bb6-75f4-4e4c-b278-ea1de69829ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49981, 6247)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- cell 2: dataset ---\n",
    "IMG_EXTS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')\n",
    "\n",
    "class ImageFolderFlat(Dataset):\n",
    "    def __init__(self, root, resize_shorter_to=512, crop_size=256):\n",
    "        self.paths = []\n",
    "        for p in sorted(Path(root).rglob(\"*\")):\n",
    "            if p.suffix.lower() in IMG_EXTS:\n",
    "                self.paths.append(str(p))\n",
    "        if not self.paths:\n",
    "            raise RuntimeError(f\"No images found under {root}\")\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Lambda(lambda im: im.convert(\"RGB\")),\n",
    "            T.Resize(resize_shorter_to, interpolation=Image.BICUBIC),\n",
    "            T.RandomCrop(crop_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx % len(self.paths)]\n",
    "        img = Image.open(p)\n",
    "        return self.transform(img)\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Iterates over content images; for each content item, pick a random style item.\n",
    "    \"\"\"\n",
    "    def __init__(self, content_root, style_root, resize_shorter_to=512, crop_size=256):\n",
    "        self.content_ds = ImageFolderFlat(content_root, resize_shorter_to, crop_size)\n",
    "        self.style_ds   = ImageFolderFlat(style_root,   resize_shorter_to, crop_size)\n",
    "        self.style_len  = len(self.style_ds)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.content_ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        content_img = self.content_ds[idx]\n",
    "        style_img   = self.style_ds[random.randrange(self.style_len)]\n",
    "        return content_img, style_img\n",
    "\n",
    "train_ds = PairDataset(cfg.content_dir, cfg.style_dir, cfg.resize_shorter_to, cfg.image_size_crop)\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True, drop_last=True)\n",
    "len(train_ds), len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "128b3a30-5ff5-45b7-bd26-083383e9668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- cell 3: vgg encoder & helpers ---\n",
    "def calc_mean_std(feat, eps=1e-5):\n",
    "    # feat: (B, C, H, W)\n",
    "    B, C = feat.size()[:2]\n",
    "    feat_var = feat.view(B, C, -1).var(dim=2, unbiased=False) + eps\n",
    "    feat_std = feat_var.sqrt().view(B, C, 1, 1)\n",
    "    feat_mean = feat.view(B, C, -1).mean(dim=2).view(B, C, 1, 1)\n",
    "    return feat_mean, feat_std\n",
    "\n",
    "def adain(content_feat, style_feat, eps=1e-5):\n",
    "    # Channel-wise align mean & std of content to style\n",
    "    size = content_feat.size()\n",
    "    c_mean, c_std = calc_mean_std(content_feat, eps)\n",
    "    s_mean, s_std = calc_mean_std(style_feat, eps)\n",
    "    normalized = (content_feat - c_mean) / c_std\n",
    "    return normalized * s_std + s_mean\n",
    "\n",
    "# Map torchvision VGG19 feature indices to friendly names\n",
    "VGG_FEATURES = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features\n",
    "\n",
    "LAYER_NAME_MAP = {\n",
    "    1 : 'relu1_1',\n",
    "    6 : 'relu2_1',\n",
    "    11: 'relu3_1',\n",
    "    20: 'relu4_1',\n",
    "}\n",
    "\n",
    "STYLE_LAYERS = ['relu1_1','relu2_1','relu3_1','relu4_1']\n",
    "CONTENT_LAYER = 'relu4_1'  # for AdaIN content loss\n",
    "\n",
    "class VGGEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG-19 (imagenet) up to relu4_1. Returns a dict of selected layer activations.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features\n",
    "        # Freeze params\n",
    "        for p in self.vgg.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x, out_keys=('relu1_1','relu2_1','relu3_1','relu4_1')):\n",
    "        feats = {}\n",
    "        h = x\n",
    "        for i, layer in enumerate(self.vgg):\n",
    "            h = layer(h)\n",
    "            name = LAYER_NAME_MAP.get(i, None)\n",
    "            if name in out_keys:\n",
    "                feats[name] = h\n",
    "            if i >= 20:  # up to relu4_1\n",
    "                # We still finish this iteration to capture relu4_1\n",
    "                pass\n",
    "        return feats\n",
    "\n",
    "# Decoder mirrors encoder with nearest upsample, reflection padding, no norm layers\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        def block(in_c, out_c, k=3, s=1, pad='reflect', act=True):\n",
    "            if pad == 'reflect':\n",
    "                layers.append(nn.ReflectionPad2d((k-1)//2))\n",
    "                layers.append(nn.Conv2d(in_c, out_c, kernel_size=k, stride=s, padding=0))\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(in_c, out_c, kernel_size=k, stride=s, padding=(k-1)//2))\n",
    "            if act:\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        # Start from relu4_1 feature space (512 channels)\n",
    "        # Mirror path back to image (3 channels)\n",
    "        # A common decoder matching VGG blocks (no norm, reflection pads, NN upsample)\n",
    "        self.body = nn.Sequential(\n",
    "            # relu4_1 -> block3\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(512, 256, 3), nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(256, 256, 3), nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(256, 256, 3), nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(256, 256, 3), nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(256, 128, 3), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(128, 128, 3), nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(128, 64, 3), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(64, 64, 3), nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(64, 3, 3)  # last conv, no activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "# Loss network wrapper using VGG-19 (fixed)\n",
    "class LossNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = VGGEncoder()\n",
    "        self.encoder.eval()\n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, x, keys=None):\n",
    "        return self.encoder(x, out_keys=tuple(keys) if keys else tuple(STYLE_LAYERS))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # not used directly\n",
    "        return self.encode(x)\n",
    "\n",
    "def mean_std_loss(x_feats, y_feats):\n",
    "    \"\"\"\n",
    "    IN-statistics loss: sum over layers of (mean MSE + std MSE)\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    for k in STYLE_LAYERS:\n",
    "        xm, xs = calc_mean_std(x_feats[k])\n",
    "        ym, ys = calc_mean_std(y_feats[k])\n",
    "        loss = loss + F.mse_loss(xm, ym) + F.mse_loss(xs, ys)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81ff4980-11b8-44f7-bf83-48a2596755a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cu128\n",
      "built CUDA: 12.8\n",
      "CUDA available: True\n",
      "arch list (compiled into this wheel): ['sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90', 'sm_100', 'sm_120']\n",
      "GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "compute capability: sm_120\n",
      "Thu Aug 21 01:53:05 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch, subprocess, sys, os\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"built CUDA:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"arch list (compiled into this wheel):\", torch.cuda.get_arch_list())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    p = torch.cuda.get_device_properties(0)\n",
    "    print(\"compute capability: sm_{}{}\".format(p.major, p.minor))\n",
    "print(subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a84c910-449e-4f6f-b502-2ce79e974f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output image shape: (2, 3, 256, 256)\n",
      "Content loss: 6.061115264892578\n",
      "Style loss: 47.425384521484375\n"
     ]
    }
   ],
   "source": [
    "enc = VGGEncoder().to(device).eval()\n",
    "dec = Decoder().to(device).eval()\n",
    "lossnet = LossNet().to(device).eval()\n",
    "\n",
    "import torch.nn.functional as F\n",
    "x_content = torch.randn(2, 3, 256, 256, device=device)\n",
    "x_style   = torch.randn(2, 3, 256, 256, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c4 = enc(x_content, out_keys=['relu4_1'])['relu4_1']\n",
    "    s4 = enc(x_style,   out_keys=['relu4_1'])['relu4_1']\n",
    "    t  = adain(c4, s4)\n",
    "    y  = dec(t).clamp(-3, 3)\n",
    "    fy = lossnet.encode(y, keys=STYLE_LAYERS)\n",
    "    fs = lossnet.encode(x_style, keys=STYLE_LAYERS)\n",
    "    Lc = F.mse_loss(fy[CONTENT_LAYER], t).item()\n",
    "    Ls = mean_std_loss(fy, fs).item()\n",
    "\n",
    "print(\"Output image shape:\", tuple(y.shape))    # expect (2, 3, 256, 256)\n",
    "print(\"Content loss:\", Lc)                      # positive float\n",
    "print(\"Style loss:\", Ls)                        # positive float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce6ec49-4462-4bb9-ab55-8c364854e570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Grad sanity (decoder mean |grad|): 15.536363\n",
      "[   200/160000] loss=52.2870  Lc=12.9208  Ls=3.9366\n",
      "[   400/160000] loss=40.1096  Lc=13.9661  Ls=2.6143\n",
      "[   600/160000] loss=29.9711  Lc=11.2713  Ls=1.8700\n",
      "[   800/160000] loss=30.3923  Lc=14.3270  Ls=1.6065\n",
      "[  1000/160000] loss=24.5743  Lc=11.6843  Ls=1.2890\n",
      "[  1200/160000] loss=29.3277  Lc=10.0298  Ls=1.9298\n",
      "[  1400/160000] loss=28.0367  Lc=11.8288  Ls=1.6208\n",
      "[  1600/160000] loss=26.0955  Lc=10.5608  Ls=1.5535\n",
      "[  1800/160000] loss=27.2486  Lc=12.2926  Ls=1.4956\n",
      "[  2000/160000] loss=55.4214  Lc=16.6493  Ls=3.8772\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_2000.pth\n",
      "[  2200/160000] loss=32.3475  Lc=13.9576  Ls=1.8390\n",
      "[  2400/160000] loss=19.9653  Lc=8.1332  Ls=1.1832\n",
      "[  2600/160000] loss=39.2639  Lc=14.6628  Ls=2.4601\n",
      "[  2800/160000] loss=26.8605  Lc=10.9935  Ls=1.5867\n",
      "[  3000/160000] loss=22.3445  Lc=10.0339  Ls=1.2311\n",
      "[  3200/160000] loss=20.4982  Lc=10.3818  Ls=1.0116\n",
      "[  3400/160000] loss=20.3055  Lc=8.3495  Ls=1.1956\n",
      "[  3600/160000] loss=29.0885  Lc=13.7984  Ls=1.5290\n",
      "[  3800/160000] loss=20.1752  Lc=9.0094  Ls=1.1166\n",
      "[  4000/160000] loss=19.9917  Lc=8.5213  Ls=1.1470\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_4000.pth\n",
      "[  4200/160000] loss=22.2654  Lc=9.5772  Ls=1.2688\n",
      "[  4400/160000] loss=18.0683  Lc=8.2080  Ls=0.9860\n",
      "[  4600/160000] loss=36.0085  Lc=11.5972  Ls=2.4411\n",
      "[  4800/160000] loss=24.5182  Lc=11.5988  Ls=1.2919\n",
      "[  5000/160000] loss=26.0622  Lc=11.3621  Ls=1.4700\n",
      "[  5200/160000] loss=20.0458  Lc=9.6091  Ls=1.0437\n",
      "[  5400/160000] loss=24.7548  Lc=10.2391  Ls=1.4516\n",
      "[  5600/160000] loss=33.7261  Lc=14.5746  Ls=1.9151\n",
      "[  5800/160000] loss=21.1190  Lc=9.3070  Ls=1.1812\n",
      "[  6000/160000] loss=22.6697  Lc=11.4417  Ls=1.1228\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_6000.pth\n",
      "[  6200/160000] loss=26.6343  Lc=12.1753  Ls=1.4459\n",
      "[  6400/160000] loss=23.3054  Lc=9.8327  Ls=1.3473\n",
      "[  6600/160000] loss=18.5433  Lc=6.7912  Ls=1.1752\n",
      "[  6800/160000] loss=20.7421  Lc=10.2135  Ls=1.0529\n",
      "[  7000/160000] loss=23.0451  Lc=9.4477  Ls=1.3597\n",
      "[  7200/160000] loss=14.8629  Lc=6.4509  Ls=0.8412\n",
      "[  7400/160000] loss=20.5607  Lc=9.2891  Ls=1.1272\n",
      "[  7600/160000] loss=21.0453  Lc=8.6787  Ls=1.2367\n",
      "[  7800/160000] loss=20.0672  Lc=8.4931  Ls=1.1574\n",
      "[  8000/160000] loss=21.2688  Lc=9.9818  Ls=1.1287\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_8000.pth\n",
      "[  8200/160000] loss=19.3335  Lc=8.9602  Ls=1.0373\n",
      "[  8400/160000] loss=23.0884  Lc=9.1548  Ls=1.3934\n",
      "[  8600/160000] loss=18.3212  Lc=7.6879  Ls=1.0633\n",
      "[  8800/160000] loss=33.6286  Lc=13.2758  Ls=2.0353\n",
      "[  9000/160000] loss=14.4647  Lc=6.5625  Ls=0.7902\n",
      "[  9200/160000] loss=19.7460  Lc=8.5326  Ls=1.1213\n",
      "[  9400/160000] loss=18.0210  Lc=8.2715  Ls=0.9749\n",
      "[  9600/160000] loss=23.0495  Lc=10.7282  Ls=1.2321\n",
      "[  9800/160000] loss=19.9271  Lc=8.0361  Ls=1.1891\n",
      "[ 10000/160000] loss=16.6355  Lc=7.7088  Ls=0.8927\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_10000.pth\n",
      "[ 10200/160000] loss=15.7660  Lc=7.4150  Ls=0.8351\n",
      "[ 10400/160000] loss=17.2825  Lc=8.0064  Ls=0.9276\n",
      "[ 10600/160000] loss=16.8905  Lc=7.7914  Ls=0.9099\n",
      "[ 10800/160000] loss=18.8931  Lc=9.0275  Ls=0.9866\n",
      "[ 11000/160000] loss=19.1227  Lc=8.4319  Ls=1.0691\n",
      "[ 11200/160000] loss=18.3383  Lc=8.0122  Ls=1.0326\n",
      "[ 11400/160000] loss=18.9762  Lc=8.5138  Ls=1.0462\n",
      "[ 11600/160000] loss=18.2398  Lc=8.9170  Ls=0.9323\n",
      "[ 11800/160000] loss=23.4717  Lc=9.6923  Ls=1.3779\n",
      "[ 12000/160000] loss=13.5392  Lc=6.1791  Ls=0.7360\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_12000.pth\n",
      "[ 12200/160000] loss=15.9505  Lc=6.7036  Ls=0.9247\n",
      "[ 12400/160000] loss=19.2983  Lc=9.3718  Ls=0.9926\n",
      "[ 12600/160000] loss=21.9991  Lc=9.6468  Ls=1.2352\n",
      "[ 12800/160000] loss=27.5112  Lc=11.1694  Ls=1.6342\n",
      "[ 13000/160000] loss=28.8758  Lc=10.8553  Ls=1.8021\n",
      "[ 13200/160000] loss=18.0144  Lc=7.7758  Ls=1.0239\n",
      "[ 13400/160000] loss=11.8808  Lc=4.9773  Ls=0.6903\n",
      "[ 13600/160000] loss=23.3872  Lc=12.1227  Ls=1.1264\n",
      "[ 13800/160000] loss=22.3187  Lc=8.7378  Ls=1.3581\n",
      "[ 14000/160000] loss=14.0250  Lc=6.3808  Ls=0.7644\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_14000.pth\n",
      "[ 14200/160000] loss=20.0352  Lc=8.9676  Ls=1.1068\n",
      "[ 14400/160000] loss=16.2295  Lc=7.6211  Ls=0.8608\n",
      "[ 14600/160000] loss=15.1588  Lc=7.1432  Ls=0.8016\n",
      "[ 14800/160000] loss=18.2313  Lc=8.2077  Ls=1.0024\n",
      "[ 15000/160000] loss=17.9921  Lc=8.8324  Ls=0.9160\n",
      "[ 15200/160000] loss=34.3406  Lc=13.4575  Ls=2.0883\n",
      "[ 15400/160000] loss=13.3179  Lc=6.5192  Ls=0.6799\n",
      "[ 15600/160000] loss=27.6653  Lc=11.7243  Ls=1.5941\n",
      "[ 15800/160000] loss=12.7961  Lc=5.8005  Ls=0.6996\n",
      "[ 16000/160000] loss=18.9380  Lc=9.8099  Ls=0.9128\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_16000.pth\n",
      "[ 16200/160000] loss=19.2076  Lc=9.5342  Ls=0.9673\n",
      "[ 16400/160000] loss=21.5407  Lc=11.0110  Ls=1.0530\n",
      "[ 16600/160000] loss=16.5168  Lc=8.3642  Ls=0.8153\n",
      "[ 16800/160000] loss=21.4880  Lc=10.0619  Ls=1.1426\n",
      "[ 17000/160000] loss=15.9478  Lc=7.7171  Ls=0.8231\n",
      "[ 17200/160000] loss=15.6301  Lc=7.0935  Ls=0.8537\n",
      "[ 17400/160000] loss=20.2570  Lc=10.2885  Ls=0.9968\n",
      "[ 17600/160000] loss=15.3660  Lc=7.7930  Ls=0.7573\n",
      "[ 17800/160000] loss=15.3378  Lc=7.6368  Ls=0.7701\n",
      "[ 18000/160000] loss=18.8978  Lc=9.5150  Ls=0.9383\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_18000.pth\n",
      "[ 18200/160000] loss=25.0364  Lc=11.3247  Ls=1.3712\n",
      "[ 18400/160000] loss=16.2549  Lc=8.0697  Ls=0.8185\n",
      "[ 18600/160000] loss=20.8895  Lc=10.5314  Ls=1.0358\n",
      "[ 18800/160000] loss=21.1787  Lc=10.2591  Ls=1.0920\n",
      "[ 19000/160000] loss=19.6032  Lc=9.6238  Ls=0.9979\n",
      "[ 19200/160000] loss=18.6645  Lc=9.1104  Ls=0.9554\n",
      "[ 19400/160000] loss=13.8635  Lc=7.5092  Ls=0.6354\n",
      "[ 19600/160000] loss=14.2369  Lc=6.5095  Ls=0.7727\n",
      "[ 19800/160000] loss=19.4416  Lc=9.1235  Ls=1.0318\n",
      "[ 20000/160000] loss=22.2917  Lc=7.2804  Ls=1.5011\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_20000.pth\n",
      "[ 20200/160000] loss=15.9338  Lc=8.2216  Ls=0.7712\n",
      "[ 20400/160000] loss=17.1237  Lc=7.8054  Ls=0.9318\n",
      "[ 20600/160000] loss=17.0281  Lc=7.9010  Ls=0.9127\n",
      "[ 20800/160000] loss=16.4546  Lc=7.3361  Ls=0.9119\n",
      "[ 21000/160000] loss=22.1771  Lc=9.7141  Ls=1.2463\n",
      "[ 21200/160000] loss=21.8457  Lc=9.2715  Ls=1.2574\n",
      "[ 21400/160000] loss=20.7957  Lc=9.1583  Ls=1.1637\n",
      "[ 21600/160000] loss=13.7634  Lc=7.0892  Ls=0.6674\n",
      "[ 21800/160000] loss=23.1596  Lc=10.9639  Ls=1.2196\n",
      "[ 22000/160000] loss=19.6611  Lc=10.3330  Ls=0.9328\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_22000.pth\n",
      "[ 22200/160000] loss=22.9128  Lc=10.3856  Ls=1.2527\n",
      "[ 22400/160000] loss=16.3079  Lc=7.4178  Ls=0.8890\n",
      "[ 22600/160000] loss=13.9231  Lc=6.7678  Ls=0.7155\n",
      "[ 22800/160000] loss=17.8000  Lc=8.0984  Ls=0.9702\n",
      "[ 23000/160000] loss=31.1386  Lc=12.0673  Ls=1.9071\n",
      "[ 23200/160000] loss=19.5708  Lc=9.6781  Ls=0.9893\n",
      "[ 23400/160000] loss=17.3183  Lc=7.8952  Ls=0.9423\n",
      "[ 23600/160000] loss=20.9222  Lc=9.6605  Ls=1.1262\n",
      "[ 23800/160000] loss=19.7074  Lc=9.7037  Ls=1.0004\n",
      "[ 24000/160000] loss=17.6079  Lc=8.3377  Ls=0.9270\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_24000.pth\n",
      "[ 24200/160000] loss=17.6903  Lc=8.0254  Ls=0.9665\n",
      "[ 24400/160000] loss=17.4870  Lc=7.8479  Ls=0.9639\n",
      "[ 24600/160000] loss=16.8687  Lc=8.9941  Ls=0.7875\n",
      "[ 24800/160000] loss=14.0286  Lc=7.5547  Ls=0.6474\n",
      "[ 25000/160000] loss=13.6579  Lc=6.7009  Ls=0.6957\n",
      "[ 25200/160000] loss=16.7960  Lc=8.4839  Ls=0.8312\n",
      "[ 25400/160000] loss=12.8279  Lc=5.9596  Ls=0.6868\n",
      "[ 25600/160000] loss=31.2807  Lc=14.8918  Ls=1.6389\n",
      "[ 25800/160000] loss=18.8569  Lc=8.7185  Ls=1.0138\n",
      "[ 26000/160000] loss=14.8678  Lc=6.2783  Ls=0.8590\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_26000.pth\n",
      "[ 26200/160000] loss=16.4474  Lc=7.7655  Ls=0.8682\n",
      "[ 26400/160000] loss=10.4562  Lc=5.3035  Ls=0.5153\n",
      "[ 26600/160000] loss=20.8814  Lc=9.5160  Ls=1.1365\n",
      "[ 26800/160000] loss=20.1861  Lc=8.9916  Ls=1.1194\n",
      "[ 27000/160000] loss=19.7715  Lc=9.9234  Ls=0.9848\n",
      "[ 27200/160000] loss=12.7997  Lc=5.4932  Ls=0.7307\n",
      "[ 27400/160000] loss=14.2863  Lc=7.3131  Ls=0.6973\n",
      "[ 27600/160000] loss=14.7038  Lc=7.4915  Ls=0.7212\n",
      "[ 27800/160000] loss=15.3085  Lc=7.2927  Ls=0.8016\n",
      "[ 28000/160000] loss=9.9489  Lc=4.9045  Ls=0.5044\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_28000.pth\n",
      "[ 28200/160000] loss=18.3335  Lc=9.1455  Ls=0.9188\n",
      "[ 28400/160000] loss=20.7833  Lc=9.6457  Ls=1.1138\n",
      "[ 28600/160000] loss=12.1429  Lc=5.8124  Ls=0.6330\n",
      "[ 28800/160000] loss=23.5434  Lc=9.4572  Ls=1.4086\n",
      "[ 29000/160000] loss=16.4040  Lc=7.6565  Ls=0.8747\n",
      "[ 29200/160000] loss=11.3327  Lc=5.8310  Ls=0.5502\n",
      "[ 29400/160000] loss=26.1869  Lc=11.6055  Ls=1.4581\n",
      "[ 29600/160000] loss=13.4390  Lc=6.7308  Ls=0.6708\n",
      "[ 29800/160000] loss=14.8900  Lc=7.2371  Ls=0.7653\n",
      "[ 30000/160000] loss=30.2632  Lc=11.1887  Ls=1.9074\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_30000.pth\n",
      "[ 30200/160000] loss=12.1312  Lc=6.5383  Ls=0.5593\n",
      "[ 30400/160000] loss=16.3970  Lc=8.2119  Ls=0.8185\n",
      "[ 30600/160000] loss=19.4773  Lc=8.7678  Ls=1.0709\n",
      "[ 30800/160000] loss=11.6115  Lc=5.6660  Ls=0.5946\n",
      "[ 31000/160000] loss=17.4318  Lc=9.1451  Ls=0.8287\n",
      "[ 31200/160000] loss=13.6220  Lc=7.0564  Ls=0.6566\n",
      "[ 31400/160000] loss=13.0833  Lc=6.8557  Ls=0.6228\n",
      "[ 31600/160000] loss=53.7693  Lc=12.2441  Ls=4.1525\n",
      "[ 31800/160000] loss=14.2608  Lc=6.8797  Ls=0.7381\n",
      "[ 32000/160000] loss=14.6329  Lc=6.7559  Ls=0.7877\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_32000.pth\n",
      "[ 32200/160000] loss=17.6689  Lc=8.4115  Ls=0.9257\n",
      "[ 32400/160000] loss=17.0065  Lc=9.0889  Ls=0.7918\n",
      "[ 32600/160000] loss=13.9117  Lc=6.1979  Ls=0.7714\n",
      "[ 32800/160000] loss=18.1253  Lc=8.6267  Ls=0.9499\n",
      "[ 33000/160000] loss=14.3558  Lc=6.8157  Ls=0.7540\n",
      "[ 33200/160000] loss=24.1887  Lc=10.4916  Ls=1.3697\n",
      "[ 33400/160000] loss=17.2478  Lc=8.1352  Ls=0.9113\n",
      "[ 33600/160000] loss=15.8029  Lc=7.6132  Ls=0.8190\n",
      "[ 33800/160000] loss=12.6262  Lc=5.7501  Ls=0.6876\n",
      "[ 34000/160000] loss=15.6899  Lc=7.6318  Ls=0.8058\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_34000.pth\n",
      "[ 34200/160000] loss=13.8096  Lc=6.6490  Ls=0.7161\n",
      "[ 34400/160000] loss=13.5273  Lc=7.0772  Ls=0.6450\n",
      "[ 34600/160000] loss=22.3136  Lc=10.2573  Ls=1.2056\n",
      "[ 34800/160000] loss=20.4020  Lc=9.2332  Ls=1.1169\n",
      "[ 35000/160000] loss=13.4738  Lc=6.6591  Ls=0.6815\n",
      "[ 35200/160000] loss=17.2099  Lc=7.4974  Ls=0.9713\n",
      "[ 35400/160000] loss=15.4481  Lc=6.8486  Ls=0.8599\n",
      "[ 35600/160000] loss=16.9729  Lc=7.3019  Ls=0.9671\n",
      "[ 35800/160000] loss=18.8314  Lc=8.6110  Ls=1.0220\n",
      "[ 36000/160000] loss=17.6269  Lc=8.8103  Ls=0.8817\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_36000.pth\n",
      "[ 36200/160000] loss=19.6640  Lc=9.2323  Ls=1.0432\n",
      "[ 36400/160000] loss=12.4391  Lc=6.0399  Ls=0.6399\n",
      "[ 36600/160000] loss=17.2207  Lc=8.2680  Ls=0.8953\n",
      "[ 36800/160000] loss=14.6173  Lc=7.6432  Ls=0.6974\n",
      "[ 37000/160000] loss=11.8844  Lc=5.7139  Ls=0.6170\n",
      "[ 37200/160000] loss=12.1650  Lc=6.2088  Ls=0.5956\n",
      "[ 37400/160000] loss=15.1621  Lc=7.6530  Ls=0.7509\n",
      "[ 37600/160000] loss=16.4024  Lc=7.3949  Ls=0.9008\n",
      "[ 37800/160000] loss=13.5559  Lc=5.9708  Ls=0.7585\n",
      "[ 38000/160000] loss=14.3049  Lc=6.0548  Ls=0.8250\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_38000.pth\n",
      "[ 38200/160000] loss=21.7600  Lc=10.5000  Ls=1.1260\n",
      "[ 38400/160000] loss=21.0819  Lc=8.9358  Ls=1.2146\n",
      "[ 38600/160000] loss=9.5173  Lc=5.0548  Ls=0.4462\n",
      "[ 38800/160000] loss=16.5584  Lc=6.8649  Ls=0.9694\n",
      "[ 39000/160000] loss=19.1313  Lc=8.6066  Ls=1.0525\n",
      "[ 39200/160000] loss=14.1375  Lc=7.2906  Ls=0.6847\n",
      "[ 39400/160000] loss=15.5260  Lc=7.4614  Ls=0.8065\n",
      "[ 39600/160000] loss=15.5729  Lc=6.7371  Ls=0.8836\n",
      "[ 39800/160000] loss=14.4174  Lc=7.2596  Ls=0.7158\n",
      "[ 40000/160000] loss=16.2727  Lc=8.3215  Ls=0.7951\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_40000.pth\n",
      "[ 40200/160000] loss=15.8626  Lc=7.3449  Ls=0.8518\n",
      "[ 40400/160000] loss=21.4825  Lc=10.8089  Ls=1.0674\n",
      "[ 40600/160000] loss=10.7865  Lc=5.3981  Ls=0.5388\n",
      "[ 40800/160000] loss=8.4862  Lc=4.3850  Ls=0.4101\n",
      "[ 41000/160000] loss=15.7316  Lc=8.2605  Ls=0.7471\n",
      "[ 41200/160000] loss=16.1634  Lc=7.9932  Ls=0.8170\n",
      "[ 41400/160000] loss=14.3780  Lc=7.3642  Ls=0.7014\n",
      "[ 41600/160000] loss=16.1584  Lc=8.0161  Ls=0.8142\n",
      "[ 41800/160000] loss=22.1091  Lc=10.1437  Ls=1.1965\n",
      "[ 42000/160000] loss=14.5905  Lc=6.9397  Ls=0.7651\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_42000.pth\n",
      "[ 42200/160000] loss=19.0683  Lc=9.2805  Ls=0.9788\n",
      "[ 42400/160000] loss=12.3073  Lc=6.1206  Ls=0.6187\n",
      "[ 42600/160000] loss=19.6552  Lc=8.3396  Ls=1.1316\n",
      "[ 42800/160000] loss=22.9575  Lc=9.1559  Ls=1.3802\n",
      "[ 43000/160000] loss=12.9398  Lc=6.0956  Ls=0.6844\n",
      "[ 43200/160000] loss=13.6933  Lc=6.9242  Ls=0.6769\n",
      "[ 43400/160000] loss=14.9595  Lc=7.6529  Ls=0.7307\n",
      "[ 43600/160000] loss=27.7019  Lc=11.6087  Ls=1.6093\n",
      "[ 43800/160000] loss=13.1417  Lc=6.8599  Ls=0.6282\n",
      "[ 44000/160000] loss=14.0744  Lc=7.1815  Ls=0.6893\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_44000.pth\n",
      "[ 44200/160000] loss=13.9562  Lc=7.1125  Ls=0.6844\n",
      "[ 44400/160000] loss=14.1295  Lc=6.5264  Ls=0.7603\n",
      "[ 44600/160000] loss=16.5689  Lc=6.7827  Ls=0.9786\n",
      "[ 44800/160000] loss=10.2367  Lc=5.1893  Ls=0.5047\n",
      "[ 45000/160000] loss=14.4468  Lc=7.0768  Ls=0.7370\n",
      "[ 45200/160000] loss=16.4206  Lc=8.0991  Ls=0.8321\n",
      "[ 45400/160000] loss=16.8215  Lc=8.2019  Ls=0.8620\n",
      "[ 45600/160000] loss=22.9869  Lc=10.8149  Ls=1.2172\n",
      "[ 45800/160000] loss=20.3254  Lc=8.6705  Ls=1.1655\n",
      "[ 46000/160000] loss=14.3449  Lc=7.4185  Ls=0.6926\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_46000.pth\n",
      "[ 46200/160000] loss=23.8515  Lc=10.6064  Ls=1.3245\n",
      "[ 46400/160000] loss=18.1260  Lc=9.6932  Ls=0.8433\n",
      "[ 46600/160000] loss=22.6152  Lc=10.7709  Ls=1.1844\n",
      "[ 46800/160000] loss=18.7484  Lc=8.7941  Ls=0.9954\n",
      "[ 47000/160000] loss=15.4186  Lc=7.8607  Ls=0.7558\n",
      "[ 47200/160000] loss=21.3903  Lc=9.5266  Ls=1.1864\n",
      "[ 47400/160000] loss=11.6125  Lc=5.8248  Ls=0.5788\n",
      "[ 47600/160000] loss=12.8749  Lc=6.2221  Ls=0.6653\n",
      "[ 47800/160000] loss=19.5819  Lc=9.7844  Ls=0.9797\n",
      "[ 48000/160000] loss=12.0745  Lc=6.1530  Ls=0.5921\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_48000.pth\n",
      "[ 48200/160000] loss=21.4932  Lc=9.8070  Ls=1.1686\n",
      "[ 48400/160000] loss=15.7158  Lc=8.5807  Ls=0.7135\n",
      "[ 48600/160000] loss=13.5481  Lc=6.2722  Ls=0.7276\n",
      "[ 48800/160000] loss=20.9883  Lc=10.8822  Ls=1.0106\n",
      "[ 49000/160000] loss=12.3127  Lc=5.6075  Ls=0.6705\n",
      "[ 49200/160000] loss=12.8945  Lc=6.3999  Ls=0.6495\n",
      "[ 49400/160000] loss=25.1346  Lc=9.9111  Ls=1.5224\n",
      "[ 49600/160000] loss=18.2593  Lc=8.2894  Ls=0.9970\n",
      "[ 49800/160000] loss=12.0340  Lc=5.6228  Ls=0.6411\n",
      "[ 50000/160000] loss=14.3290  Lc=6.9991  Ls=0.7330\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_50000.pth\n",
      "[ 50200/160000] loss=12.2062  Lc=5.4305  Ls=0.6776\n",
      "[ 50400/160000] loss=16.5836  Lc=7.6148  Ls=0.8969\n",
      "[ 50600/160000] loss=19.7919  Lc=8.4897  Ls=1.1302\n",
      "[ 50800/160000] loss=10.1693  Lc=5.0079  Ls=0.5161\n",
      "[ 51000/160000] loss=15.6963  Lc=7.2107  Ls=0.8486\n",
      "[ 51200/160000] loss=16.0896  Lc=7.6550  Ls=0.8435\n",
      "[ 51400/160000] loss=14.8104  Lc=6.4949  Ls=0.8315\n",
      "[ 51600/160000] loss=16.4114  Lc=7.6597  Ls=0.8752\n",
      "[ 51800/160000] loss=12.7851  Lc=6.7251  Ls=0.6060\n",
      "[ 52000/160000] loss=22.6447  Lc=11.4476  Ls=1.1197\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_52000.pth\n",
      "[ 52200/160000] loss=17.1227  Lc=7.6709  Ls=0.9452\n",
      "[ 52400/160000] loss=12.0840  Lc=5.9737  Ls=0.6110\n",
      "[ 52600/160000] loss=16.2439  Lc=8.3346  Ls=0.7909\n",
      "[ 52800/160000] loss=14.8897  Lc=7.1152  Ls=0.7774\n",
      "[ 53000/160000] loss=16.2296  Lc=8.2328  Ls=0.7997\n",
      "[ 53200/160000] loss=17.2481  Lc=8.4885  Ls=0.8760\n",
      "[ 53400/160000] loss=16.3475  Lc=7.3854  Ls=0.8962\n",
      "[ 53600/160000] loss=15.0577  Lc=6.7344  Ls=0.8323\n",
      "[ 53800/160000] loss=13.1435  Lc=6.7739  Ls=0.6370\n",
      "[ 54000/160000] loss=13.4668  Lc=6.6809  Ls=0.6786\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_54000.pth\n",
      "[ 54200/160000] loss=21.0127  Lc=10.7416  Ls=1.0271\n",
      "[ 54400/160000] loss=13.9369  Lc=7.6225  Ls=0.6314\n",
      "[ 54600/160000] loss=24.0790  Lc=8.3896  Ls=1.5689\n",
      "[ 54800/160000] loss=17.3324  Lc=8.4733  Ls=0.8859\n",
      "[ 55000/160000] loss=11.1978  Lc=5.0321  Ls=0.6166\n",
      "[ 55200/160000] loss=17.7613  Lc=8.3380  Ls=0.9423\n",
      "[ 55400/160000] loss=9.8380  Lc=4.7906  Ls=0.5047\n",
      "[ 55600/160000] loss=21.4998  Lc=10.5783  Ls=1.0922\n",
      "[ 55800/160000] loss=13.1820  Lc=6.3701  Ls=0.6812\n",
      "[ 56000/160000] loss=10.5804  Lc=4.9283  Ls=0.5652\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_56000.pth\n",
      "[ 56200/160000] loss=17.8141  Lc=8.1785  Ls=0.9636\n",
      "[ 56400/160000] loss=13.0734  Lc=5.7831  Ls=0.7290\n",
      "[ 56600/160000] loss=13.7757  Lc=6.9764  Ls=0.6799\n",
      "[ 56800/160000] loss=13.1116  Lc=6.8362  Ls=0.6275\n",
      "[ 57000/160000] loss=15.8107  Lc=7.3177  Ls=0.8493\n",
      "[ 57200/160000] loss=12.8528  Lc=6.1276  Ls=0.6725\n",
      "[ 57400/160000] loss=18.7933  Lc=8.6240  Ls=1.0169\n",
      "[ 57600/160000] loss=16.9852  Lc=8.0494  Ls=0.8936\n",
      "[ 57800/160000] loss=16.1839  Lc=7.9656  Ls=0.8218\n",
      "[ 58000/160000] loss=18.1855  Lc=8.8312  Ls=0.9354\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_58000.pth\n",
      "[ 58200/160000] loss=13.8476  Lc=6.6410  Ls=0.7207\n",
      "[ 58400/160000] loss=18.4662  Lc=9.7957  Ls=0.8671\n",
      "[ 58600/160000] loss=13.3693  Lc=6.3373  Ls=0.7032\n",
      "[ 58800/160000] loss=17.1881  Lc=7.5079  Ls=0.9680\n",
      "[ 59000/160000] loss=13.6357  Lc=6.1135  Ls=0.7522\n",
      "[ 59200/160000] loss=13.8166  Lc=6.5962  Ls=0.7220\n",
      "[ 59400/160000] loss=14.2969  Lc=6.5781  Ls=0.7719\n",
      "[ 59600/160000] loss=19.0249  Lc=7.5417  Ls=1.1483\n",
      "[ 59800/160000] loss=11.6641  Lc=5.6539  Ls=0.6010\n",
      "[ 60000/160000] loss=12.5037  Lc=6.1670  Ls=0.6337\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_60000.pth\n",
      "[ 60200/160000] loss=18.3224  Lc=9.4053  Ls=0.8917\n",
      "[ 60400/160000] loss=16.6007  Lc=8.3139  Ls=0.8287\n",
      "[ 60600/160000] loss=11.4997  Lc=5.1362  Ls=0.6363\n",
      "[ 60800/160000] loss=12.9644  Lc=6.3662  Ls=0.6598\n",
      "[ 61000/160000] loss=25.8594  Lc=12.4399  Ls=1.3419\n",
      "[ 61200/160000] loss=10.7353  Lc=5.6557  Ls=0.5080\n",
      "[ 61400/160000] loss=15.5219  Lc=7.8849  Ls=0.7637\n",
      "[ 61600/160000] loss=11.0138  Lc=5.1140  Ls=0.5900\n",
      "[ 61800/160000] loss=20.7652  Lc=8.2166  Ls=1.2549\n",
      "[ 62000/160000] loss=20.8988  Lc=7.3110  Ls=1.3588\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_62000.pth\n",
      "[ 62200/160000] loss=15.5314  Lc=8.0461  Ls=0.7485\n",
      "[ 62400/160000] loss=13.4305  Lc=6.2094  Ls=0.7221\n",
      "[ 62600/160000] loss=12.2958  Lc=6.6840  Ls=0.5612\n",
      "[ 62800/160000] loss=12.1543  Lc=6.2965  Ls=0.5858\n",
      "[ 63000/160000] loss=12.0846  Lc=5.6212  Ls=0.6463\n",
      "[ 63200/160000] loss=13.2742  Lc=6.5250  Ls=0.6749\n",
      "[ 63400/160000] loss=13.1284  Lc=6.7078  Ls=0.6421\n",
      "[ 63600/160000] loss=13.9048  Lc=6.7224  Ls=0.7182\n",
      "[ 63800/160000] loss=24.0509  Lc=9.5191  Ls=1.4532\n",
      "[ 64000/160000] loss=14.7334  Lc=7.1855  Ls=0.7548\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_64000.pth\n",
      "[ 64200/160000] loss=17.7609  Lc=9.1198  Ls=0.8641\n",
      "[ 64400/160000] loss=18.7064  Lc=9.4764  Ls=0.9230\n",
      "[ 64600/160000] loss=11.0878  Lc=5.5234  Ls=0.5564\n",
      "[ 64800/160000] loss=12.3956  Lc=6.0090  Ls=0.6387\n",
      "[ 65000/160000] loss=19.5664  Lc=9.0688  Ls=1.0498\n",
      "[ 65200/160000] loss=17.7060  Lc=9.5490  Ls=0.8157\n",
      "[ 65400/160000] loss=9.9803  Lc=5.3032  Ls=0.4677\n",
      "[ 65600/160000] loss=14.1406  Lc=6.6411  Ls=0.7499\n",
      "[ 65800/160000] loss=11.3251  Lc=5.9539  Ls=0.5371\n",
      "[ 66000/160000] loss=16.3865  Lc=7.0198  Ls=0.9367\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_66000.pth\n",
      "[ 66200/160000] loss=21.5078  Lc=10.4236  Ls=1.1084\n",
      "[ 66400/160000] loss=14.1686  Lc=6.5604  Ls=0.7608\n",
      "[ 66600/160000] loss=12.8756  Lc=6.8385  Ls=0.6037\n",
      "[ 66800/160000] loss=13.1204  Lc=6.6672  Ls=0.6453\n",
      "[ 67000/160000] loss=14.1556  Lc=7.4013  Ls=0.6754\n",
      "[ 67200/160000] loss=9.1894  Lc=4.4190  Ls=0.4770\n",
      "[ 67400/160000] loss=16.1410  Lc=7.9980  Ls=0.8143\n",
      "[ 67600/160000] loss=10.6562  Lc=5.6514  Ls=0.5005\n",
      "[ 67800/160000] loss=13.7893  Lc=6.2935  Ls=0.7496\n",
      "[ 68000/160000] loss=11.9656  Lc=5.2831  Ls=0.6682\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_68000.pth\n",
      "[ 68200/160000] loss=11.8722  Lc=5.8224  Ls=0.6050\n",
      "[ 68400/160000] loss=14.3053  Lc=6.0298  Ls=0.8276\n",
      "[ 68600/160000] loss=19.9840  Lc=9.2393  Ls=1.0745\n",
      "[ 68800/160000] loss=14.3580  Lc=7.3078  Ls=0.7050\n",
      "[ 69000/160000] loss=12.7588  Lc=5.6013  Ls=0.7157\n",
      "[ 69200/160000] loss=14.2127  Lc=7.7660  Ls=0.6447\n",
      "[ 69400/160000] loss=21.4858  Lc=9.2184  Ls=1.2267\n",
      "[ 69600/160000] loss=14.3476  Lc=6.8822  Ls=0.7465\n",
      "[ 69800/160000] loss=10.9296  Lc=5.6145  Ls=0.5315\n",
      "[ 70000/160000] loss=13.3322  Lc=6.3181  Ls=0.7014\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_70000.pth\n",
      "[ 70200/160000] loss=12.8693  Lc=5.9790  Ls=0.6890\n",
      "[ 70400/160000] loss=11.0647  Lc=5.8781  Ls=0.5187\n",
      "[ 70600/160000] loss=16.0693  Lc=8.2190  Ls=0.7850\n",
      "[ 70800/160000] loss=8.9383  Lc=4.5579  Ls=0.4380\n",
      "[ 71000/160000] loss=21.8388  Lc=8.3724  Ls=1.3466\n",
      "[ 71200/160000] loss=10.3773  Lc=4.4269  Ls=0.5950\n",
      "[ 71400/160000] loss=13.1810  Lc=6.6799  Ls=0.6501\n",
      "[ 71600/160000] loss=14.8938  Lc=8.0736  Ls=0.6820\n",
      "[ 71800/160000] loss=14.7423  Lc=7.7927  Ls=0.6950\n",
      "[ 72000/160000] loss=17.5187  Lc=8.5226  Ls=0.8996\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_72000.pth\n",
      "[ 72200/160000] loss=10.9160  Lc=5.3162  Ls=0.5600\n",
      "[ 72400/160000] loss=14.7278  Lc=7.0812  Ls=0.7647\n",
      "[ 72600/160000] loss=15.5690  Lc=7.2414  Ls=0.8328\n",
      "[ 72800/160000] loss=17.5235  Lc=9.0804  Ls=0.8443\n",
      "[ 73000/160000] loss=15.8741  Lc=8.6490  Ls=0.7225\n",
      "[ 73200/160000] loss=40.3197  Lc=12.2191  Ls=2.8101\n",
      "[ 73400/160000] loss=13.2587  Lc=6.8935  Ls=0.6365\n",
      "[ 73600/160000] loss=13.2690  Lc=6.3642  Ls=0.6905\n",
      "[ 73800/160000] loss=13.6348  Lc=6.4368  Ls=0.7198\n",
      "[ 74000/160000] loss=13.2630  Lc=6.5686  Ls=0.6694\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_74000.pth\n",
      "[ 74200/160000] loss=15.8025  Lc=7.5859  Ls=0.8217\n",
      "[ 74400/160000] loss=15.8134  Lc=8.3374  Ls=0.7476\n",
      "[ 74600/160000] loss=13.9786  Lc=6.9297  Ls=0.7049\n",
      "[ 74800/160000] loss=16.8253  Lc=8.0643  Ls=0.8761\n",
      "[ 75000/160000] loss=13.7497  Lc=6.3474  Ls=0.7402\n",
      "[ 75200/160000] loss=15.3858  Lc=8.1243  Ls=0.7262\n",
      "[ 75400/160000] loss=14.0091  Lc=6.6853  Ls=0.7324\n",
      "[ 75600/160000] loss=16.1970  Lc=8.4477  Ls=0.7749\n",
      "[ 75800/160000] loss=11.5581  Lc=6.0223  Ls=0.5536\n",
      "[ 76000/160000] loss=14.3746  Lc=6.6679  Ls=0.7707\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_76000.pth\n",
      "[ 76200/160000] loss=15.0983  Lc=7.5750  Ls=0.7523\n",
      "[ 76400/160000] loss=16.1891  Lc=7.9226  Ls=0.8267\n",
      "[ 76600/160000] loss=13.6123  Lc=6.4486  Ls=0.7164\n",
      "[ 76800/160000] loss=13.5025  Lc=6.1665  Ls=0.7336\n",
      "[ 77000/160000] loss=14.7316  Lc=7.2627  Ls=0.7469\n",
      "[ 77200/160000] loss=14.0704  Lc=6.0542  Ls=0.8016\n",
      "[ 77400/160000] loss=14.4343  Lc=6.7544  Ls=0.7680\n",
      "[ 77600/160000] loss=18.2629  Lc=9.5131  Ls=0.8750\n",
      "[ 77800/160000] loss=11.2923  Lc=5.6022  Ls=0.5690\n",
      "[ 78000/160000] loss=13.3750  Lc=6.7162  Ls=0.6659\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_78000.pth\n",
      "[ 78200/160000] loss=12.2257  Lc=6.1253  Ls=0.6100\n",
      "[ 78400/160000] loss=12.3348  Lc=5.8916  Ls=0.6443\n",
      "[ 78600/160000] loss=14.4244  Lc=6.8501  Ls=0.7574\n",
      "[ 78800/160000] loss=10.4221  Lc=5.6504  Ls=0.4772\n",
      "[ 79000/160000] loss=13.5116  Lc=7.0549  Ls=0.6457\n",
      "[ 79200/160000] loss=22.5409  Lc=10.7517  Ls=1.1789\n",
      "[ 79400/160000] loss=16.8901  Lc=8.2047  Ls=0.8685\n",
      "[ 79600/160000] loss=17.4258  Lc=8.8269  Ls=0.8599\n",
      "[ 79800/160000] loss=12.2852  Lc=6.0738  Ls=0.6211\n",
      "[ 80000/160000] loss=11.4779  Lc=5.7057  Ls=0.5772\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_80000.pth\n",
      "[ 80200/160000] loss=9.7594  Lc=4.8401  Ls=0.4919\n",
      "[ 80400/160000] loss=14.7814  Lc=6.4130  Ls=0.8368\n",
      "[ 80600/160000] loss=17.9842  Lc=8.2813  Ls=0.9703\n",
      "[ 80800/160000] loss=15.1872  Lc=8.0236  Ls=0.7164\n",
      "[ 81000/160000] loss=9.6943  Lc=5.0041  Ls=0.4690\n",
      "[ 81200/160000] loss=15.7156  Lc=7.9821  Ls=0.7734\n",
      "[ 81400/160000] loss=12.4944  Lc=5.8797  Ls=0.6615\n",
      "[ 81600/160000] loss=11.0942  Lc=5.7070  Ls=0.5387\n",
      "[ 81800/160000] loss=13.0163  Lc=6.5845  Ls=0.6432\n",
      "[ 82000/160000] loss=17.9745  Lc=8.6008  Ls=0.9374\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_82000.pth\n",
      "[ 82200/160000] loss=11.3560  Lc=5.7704  Ls=0.5586\n",
      "[ 82400/160000] loss=13.8869  Lc=6.8353  Ls=0.7052\n",
      "[ 82600/160000] loss=12.5239  Lc=6.2300  Ls=0.6294\n",
      "[ 82800/160000] loss=13.5927  Lc=6.5260  Ls=0.7067\n",
      "[ 83000/160000] loss=14.2671  Lc=7.2057  Ls=0.7061\n",
      "[ 83200/160000] loss=14.1739  Lc=6.9515  Ls=0.7222\n",
      "[ 83400/160000] loss=11.1442  Lc=5.6962  Ls=0.5448\n",
      "[ 83600/160000] loss=14.2946  Lc=6.3611  Ls=0.7934\n",
      "[ 83800/160000] loss=10.8263  Lc=5.6513  Ls=0.5175\n",
      "[ 84000/160000] loss=13.6806  Lc=6.5940  Ls=0.7087\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_84000.pth\n",
      "[ 84200/160000] loss=15.9009  Lc=7.3998  Ls=0.8501\n",
      "[ 84400/160000] loss=13.7782  Lc=6.5214  Ls=0.7257\n",
      "[ 84600/160000] loss=14.6168  Lc=7.1339  Ls=0.7483\n",
      "[ 84800/160000] loss=17.7655  Lc=8.0946  Ls=0.9671\n",
      "[ 85000/160000] loss=13.8894  Lc=6.8509  Ls=0.7039\n",
      "[ 85200/160000] loss=17.1755  Lc=8.3337  Ls=0.8842\n",
      "[ 85400/160000] loss=17.5836  Lc=7.2577  Ls=1.0326\n",
      "[ 85600/160000] loss=11.2210  Lc=5.3072  Ls=0.5914\n",
      "[ 85800/160000] loss=13.9952  Lc=6.2993  Ls=0.7696\n",
      "[ 86000/160000] loss=11.6302  Lc=4.8436  Ls=0.6787\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_86000.pth\n",
      "[ 86200/160000] loss=13.4826  Lc=6.7048  Ls=0.6778\n",
      "[ 86400/160000] loss=11.6495  Lc=5.5781  Ls=0.6071\n",
      "[ 86600/160000] loss=17.9314  Lc=7.9900  Ls=0.9941\n",
      "[ 86800/160000] loss=11.3398  Lc=5.2211  Ls=0.6119\n",
      "[ 87000/160000] loss=19.1265  Lc=7.6632  Ls=1.1463\n",
      "[ 87200/160000] loss=13.2969  Lc=6.3914  Ls=0.6906\n",
      "[ 87400/160000] loss=10.3442  Lc=5.2725  Ls=0.5072\n",
      "[ 87600/160000] loss=16.8600  Lc=8.1689  Ls=0.8691\n",
      "[ 87800/160000] loss=16.8670  Lc=8.2362  Ls=0.8631\n",
      "[ 88000/160000] loss=11.6116  Lc=6.0205  Ls=0.5591\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_88000.pth\n",
      "[ 88200/160000] loss=11.7636  Lc=5.3916  Ls=0.6372\n",
      "[ 88400/160000] loss=17.1746  Lc=8.4506  Ls=0.8724\n",
      "[ 88600/160000] loss=10.8664  Lc=5.2285  Ls=0.5638\n",
      "[ 88800/160000] loss=14.0678  Lc=6.5471  Ls=0.7521\n",
      "[ 89000/160000] loss=13.8320  Lc=7.1974  Ls=0.6635\n",
      "[ 89200/160000] loss=14.0076  Lc=6.9883  Ls=0.7019\n",
      "[ 89400/160000] loss=14.2559  Lc=6.7993  Ls=0.7457\n",
      "[ 89600/160000] loss=12.5726  Lc=6.5067  Ls=0.6066\n",
      "[ 89800/160000] loss=15.6505  Lc=7.7642  Ls=0.7886\n",
      "[ 90000/160000] loss=13.5525  Lc=6.2905  Ls=0.7262\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_90000.pth\n",
      "[ 90200/160000] loss=11.2485  Lc=5.9385  Ls=0.5310\n",
      "[ 90400/160000] loss=14.1958  Lc=7.2811  Ls=0.6915\n",
      "[ 90600/160000] loss=10.6721  Lc=5.0611  Ls=0.5611\n",
      "[ 90800/160000] loss=10.6607  Lc=5.0674  Ls=0.5593\n",
      "[ 91000/160000] loss=15.7403  Lc=7.1961  Ls=0.8544\n",
      "[ 91200/160000] loss=11.7386  Lc=6.0256  Ls=0.5713\n",
      "[ 91400/160000] loss=12.8287  Lc=6.7503  Ls=0.6078\n",
      "[ 91600/160000] loss=18.2234  Lc=8.1292  Ls=1.0094\n",
      "[ 91800/160000] loss=16.4953  Lc=7.6863  Ls=0.8809\n",
      "[ 92000/160000] loss=10.1212  Lc=4.9231  Ls=0.5198\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_92000.pth\n",
      "[ 92200/160000] loss=17.6380  Lc=8.7907  Ls=0.8847\n",
      "[ 92400/160000] loss=14.6693  Lc=7.2270  Ls=0.7442\n",
      "[ 92600/160000] loss=10.5014  Lc=5.1220  Ls=0.5379\n",
      "[ 92800/160000] loss=12.6487  Lc=5.9284  Ls=0.6720\n",
      "[ 93000/160000] loss=17.8960  Lc=8.5969  Ls=0.9299\n",
      "[ 93200/160000] loss=12.2098  Lc=5.7544  Ls=0.6455\n",
      "[ 93400/160000] loss=16.4236  Lc=6.9268  Ls=0.9497\n",
      "[ 93600/160000] loss=10.5910  Lc=5.4013  Ls=0.5190\n",
      "[ 93800/160000] loss=15.9897  Lc=8.5692  Ls=0.7420\n",
      "[ 94000/160000] loss=15.4728  Lc=7.2159  Ls=0.8257\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_94000.pth\n",
      "[ 94200/160000] loss=13.8497  Lc=7.0977  Ls=0.6752\n",
      "[ 94400/160000] loss=12.5612  Lc=6.2958  Ls=0.6265\n",
      "[ 94600/160000] loss=12.5444  Lc=6.1648  Ls=0.6380\n",
      "[ 94800/160000] loss=12.0434  Lc=6.3077  Ls=0.5736\n",
      "[ 95000/160000] loss=11.0510  Lc=5.1526  Ls=0.5898\n",
      "[ 95200/160000] loss=11.8003  Lc=6.0218  Ls=0.5779\n",
      "[ 95400/160000] loss=19.1253  Lc=8.6944  Ls=1.0431\n",
      "[ 95600/160000] loss=12.3063  Lc=5.6275  Ls=0.6679\n",
      "[ 95800/160000] loss=18.9504  Lc=8.9043  Ls=1.0046\n",
      "[ 96000/160000] loss=15.6961  Lc=7.4765  Ls=0.8220\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_96000.pth\n",
      "[ 96200/160000] loss=14.5033  Lc=6.7922  Ls=0.7711\n",
      "[ 96400/160000] loss=10.4660  Lc=5.0180  Ls=0.5448\n",
      "[ 96600/160000] loss=14.7690  Lc=7.1678  Ls=0.7601\n",
      "[ 96800/160000] loss=17.3406  Lc=8.2674  Ls=0.9073\n",
      "[ 97000/160000] loss=15.7403  Lc=8.4800  Ls=0.7260\n",
      "[ 97200/160000] loss=13.8022  Lc=6.7621  Ls=0.7040\n",
      "[ 97400/160000] loss=9.9333  Lc=4.4717  Ls=0.5462\n",
      "[ 97600/160000] loss=14.8505  Lc=7.8164  Ls=0.7034\n",
      "[ 97800/160000] loss=10.1161  Lc=5.0283  Ls=0.5088\n",
      "[ 98000/160000] loss=13.1020  Lc=6.9416  Ls=0.6160\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_98000.pth\n",
      "[ 98200/160000] loss=14.4987  Lc=7.2137  Ls=0.7285\n",
      "[ 98400/160000] loss=10.8628  Lc=5.9456  Ls=0.4917\n",
      "[ 98600/160000] loss=14.8287  Lc=6.9346  Ls=0.7894\n",
      "[ 98800/160000] loss=12.9659  Lc=6.9208  Ls=0.6045\n",
      "[ 99000/160000] loss=24.2416  Lc=11.8611  Ls=1.2380\n",
      "[ 99200/160000] loss=13.3541  Lc=6.8870  Ls=0.6467\n",
      "[ 99400/160000] loss=15.2966  Lc=7.9289  Ls=0.7368\n",
      "[ 99600/160000] loss=14.8200  Lc=7.8211  Ls=0.6999\n",
      "[ 99800/160000] loss=16.0189  Lc=6.6179  Ls=0.9401\n",
      "[100000/160000] loss=9.8064  Lc=4.8536  Ls=0.4953\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_100000.pth\n",
      "[100200/160000] loss=12.9260  Lc=5.9843  Ls=0.6942\n",
      "[100400/160000] loss=14.8398  Lc=6.9419  Ls=0.7898\n",
      "[100600/160000] loss=12.1109  Lc=6.0385  Ls=0.6072\n",
      "[100800/160000] loss=21.8195  Lc=10.0573  Ls=1.1762\n",
      "[101000/160000] loss=15.4158  Lc=7.4765  Ls=0.7939\n",
      "[101200/160000] loss=19.4794  Lc=9.0776  Ls=1.0402\n",
      "[101400/160000] loss=11.1267  Lc=5.6018  Ls=0.5525\n",
      "[101600/160000] loss=9.5784  Lc=5.1477  Ls=0.4431\n",
      "[101800/160000] loss=17.6045  Lc=8.1895  Ls=0.9415\n",
      "[102000/160000] loss=13.7275  Lc=6.6990  Ls=0.7029\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_102000.pth\n",
      "[102200/160000] loss=14.1771  Lc=6.8358  Ls=0.7341\n",
      "[102400/160000] loss=13.5848  Lc=6.6731  Ls=0.6912\n",
      "[102600/160000] loss=12.7529  Lc=6.2987  Ls=0.6454\n",
      "[102800/160000] loss=17.7066  Lc=9.7790  Ls=0.7928\n",
      "[103000/160000] loss=11.0671  Lc=6.0793  Ls=0.4988\n",
      "[103200/160000] loss=20.3261  Lc=9.9364  Ls=1.0390\n",
      "[103400/160000] loss=19.3600  Lc=9.7600  Ls=0.9600\n",
      "[103600/160000] loss=11.9818  Lc=5.1954  Ls=0.6786\n",
      "[103800/160000] loss=16.7944  Lc=8.7875  Ls=0.8007\n",
      "[104000/160000] loss=19.2192  Lc=8.5864  Ls=1.0633\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_104000.pth\n",
      "[104200/160000] loss=18.3786  Lc=8.7357  Ls=0.9643\n",
      "[104400/160000] loss=17.3312  Lc=8.6932  Ls=0.8638\n",
      "[104600/160000] loss=15.6771  Lc=7.9512  Ls=0.7726\n",
      "[104800/160000] loss=17.8145  Lc=9.0166  Ls=0.8798\n",
      "[105000/160000] loss=12.6569  Lc=6.7402  Ls=0.5917\n",
      "[105200/160000] loss=17.9095  Lc=8.7271  Ls=0.9182\n",
      "[105400/160000] loss=14.0316  Lc=6.6387  Ls=0.7393\n",
      "[105600/160000] loss=14.8245  Lc=7.5238  Ls=0.7301\n",
      "[105800/160000] loss=19.0464  Lc=9.7147  Ls=0.9332\n",
      "[106000/160000] loss=16.3988  Lc=6.9162  Ls=0.9483\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_106000.pth\n",
      "[106200/160000] loss=14.0315  Lc=7.4080  Ls=0.6624\n",
      "[106400/160000] loss=12.6410  Lc=6.6939  Ls=0.5947\n",
      "[106600/160000] loss=13.8109  Lc=6.5016  Ls=0.7309\n",
      "[106800/160000] loss=12.6232  Lc=7.0561  Ls=0.5567\n",
      "[107000/160000] loss=13.2037  Lc=6.8613  Ls=0.6342\n",
      "[107200/160000] loss=18.2807  Lc=8.5638  Ls=0.9717\n",
      "[107400/160000] loss=13.5040  Lc=6.5016  Ls=0.7002\n",
      "[107600/160000] loss=18.9223  Lc=9.7342  Ls=0.9188\n",
      "[107800/160000] loss=14.4966  Lc=7.3553  Ls=0.7141\n",
      "[108000/160000] loss=14.5075  Lc=6.5394  Ls=0.7968\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_108000.pth\n",
      "[108200/160000] loss=17.5174  Lc=9.4107  Ls=0.8107\n",
      "[108400/160000] loss=16.8608  Lc=7.6773  Ls=0.9184\n",
      "[108600/160000] loss=15.4349  Lc=7.6422  Ls=0.7793\n",
      "[108800/160000] loss=12.4701  Lc=6.0380  Ls=0.6432\n",
      "[109000/160000] loss=13.6299  Lc=7.3429  Ls=0.6287\n",
      "[109200/160000] loss=12.5313  Lc=6.5982  Ls=0.5933\n",
      "[109400/160000] loss=14.2948  Lc=7.4645  Ls=0.6830\n",
      "[109600/160000] loss=10.9600  Lc=4.9658  Ls=0.5994\n",
      "[109800/160000] loss=10.5292  Lc=5.2151  Ls=0.5314\n",
      "[110000/160000] loss=8.1123  Lc=3.7849  Ls=0.4327\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_110000.pth\n",
      "[110200/160000] loss=15.9525  Lc=8.1466  Ls=0.7806\n",
      "[110400/160000] loss=13.0623  Lc=6.4149  Ls=0.6647\n",
      "[110600/160000] loss=14.3107  Lc=7.3012  Ls=0.7010\n",
      "[110800/160000] loss=11.9230  Lc=5.5654  Ls=0.6358\n",
      "[111000/160000] loss=12.4957  Lc=6.4704  Ls=0.6025\n",
      "[111200/160000] loss=12.9436  Lc=6.7609  Ls=0.6183\n",
      "[111400/160000] loss=18.0119  Lc=9.0620  Ls=0.8950\n",
      "[111600/160000] loss=14.0347  Lc=7.0399  Ls=0.6995\n",
      "[111800/160000] loss=16.8476  Lc=8.2066  Ls=0.8641\n",
      "[112000/160000] loss=12.7814  Lc=6.1101  Ls=0.6671\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_112000.pth\n",
      "[112200/160000] loss=11.0703  Lc=6.0715  Ls=0.4999\n",
      "[112400/160000] loss=11.7661  Lc=5.7969  Ls=0.5969\n",
      "[112600/160000] loss=15.8532  Lc=6.9855  Ls=0.8868\n",
      "[112800/160000] loss=17.8414  Lc=8.9401  Ls=0.8901\n",
      "[113000/160000] loss=10.7247  Lc=5.3118  Ls=0.5413\n",
      "[113200/160000] loss=11.3798  Lc=5.8921  Ls=0.5488\n",
      "[113400/160000] loss=13.5509  Lc=7.1007  Ls=0.6450\n",
      "[113600/160000] loss=13.5764  Lc=6.5991  Ls=0.6977\n",
      "[113800/160000] loss=10.1726  Lc=5.6287  Ls=0.4544\n",
      "[114000/160000] loss=17.0079  Lc=8.6179  Ls=0.8390\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_114000.pth\n",
      "[114200/160000] loss=10.8265  Lc=5.9268  Ls=0.4900\n",
      "[114400/160000] loss=13.9729  Lc=7.6875  Ls=0.6285\n",
      "[114600/160000] loss=15.5876  Lc=7.9810  Ls=0.7607\n",
      "[114800/160000] loss=11.2335  Lc=6.0403  Ls=0.5193\n",
      "[115000/160000] loss=13.3909  Lc=6.2548  Ls=0.7136\n",
      "[115200/160000] loss=12.5641  Lc=6.6981  Ls=0.5866\n",
      "[115400/160000] loss=19.1493  Lc=9.4960  Ls=0.9653\n",
      "[115600/160000] loss=15.5853  Lc=7.3273  Ls=0.8258\n",
      "[115800/160000] loss=11.0119  Lc=5.4860  Ls=0.5526\n",
      "[116000/160000] loss=11.5260  Lc=6.2262  Ls=0.5300\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_116000.pth\n",
      "[116200/160000] loss=19.4911  Lc=8.2621  Ls=1.1229\n",
      "[116400/160000] loss=17.9208  Lc=9.4281  Ls=0.8493\n",
      "[116600/160000] loss=12.9612  Lc=6.7881  Ls=0.6173\n",
      "[116800/160000] loss=11.3423  Lc=5.8847  Ls=0.5458\n",
      "[117000/160000] loss=13.4749  Lc=6.7682  Ls=0.6707\n",
      "[117200/160000] loss=15.0085  Lc=7.9339  Ls=0.7075\n",
      "[117400/160000] loss=18.8697  Lc=8.5863  Ls=1.0283\n",
      "[117600/160000] loss=11.1803  Lc=5.3840  Ls=0.5796\n",
      "[117800/160000] loss=13.2904  Lc=6.5920  Ls=0.6698\n",
      "[118000/160000] loss=21.6607  Lc=9.2764  Ls=1.2384\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_118000.pth\n",
      "[118200/160000] loss=13.2829  Lc=6.1436  Ls=0.7139\n",
      "[118400/160000] loss=10.4058  Lc=4.9353  Ls=0.5470\n",
      "[118600/160000] loss=11.7437  Lc=5.8786  Ls=0.5865\n",
      "[118800/160000] loss=18.1104  Lc=9.8815  Ls=0.8229\n",
      "[119000/160000] loss=13.0365  Lc=6.3448  Ls=0.6692\n",
      "[119200/160000] loss=14.3959  Lc=7.7780  Ls=0.6618\n",
      "[119400/160000] loss=14.0735  Lc=7.3158  Ls=0.6758\n",
      "[119600/160000] loss=15.0261  Lc=8.2894  Ls=0.6737\n",
      "[119800/160000] loss=14.2940  Lc=6.9236  Ls=0.7370\n",
      "[120000/160000] loss=12.9935  Lc=6.8152  Ls=0.6178\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_120000.pth\n",
      "[120200/160000] loss=14.9492  Lc=6.4271  Ls=0.8522\n",
      "[120400/160000] loss=13.6528  Lc=7.3034  Ls=0.6349\n",
      "[120600/160000] loss=12.5322  Lc=5.8975  Ls=0.6635\n",
      "[120800/160000] loss=9.6954  Lc=5.3262  Ls=0.4369\n",
      "[121000/160000] loss=13.4989  Lc=6.4628  Ls=0.7036\n",
      "[121200/160000] loss=12.9163  Lc=6.3461  Ls=0.6570\n",
      "[121400/160000] loss=16.1581  Lc=7.4129  Ls=0.8745\n",
      "[121600/160000] loss=15.8335  Lc=7.7663  Ls=0.8067\n",
      "[121800/160000] loss=13.1083  Lc=7.0956  Ls=0.6013\n",
      "[122000/160000] loss=14.4537  Lc=6.9166  Ls=0.7537\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_122000.pth\n",
      "[122200/160000] loss=16.0335  Lc=7.3991  Ls=0.8634\n",
      "[122400/160000] loss=16.1152  Lc=7.9720  Ls=0.8143\n",
      "[122600/160000] loss=11.0435  Lc=5.5627  Ls=0.5481\n",
      "[122800/160000] loss=10.1940  Lc=4.6674  Ls=0.5527\n",
      "[123000/160000] loss=13.7181  Lc=6.7029  Ls=0.7015\n",
      "[123200/160000] loss=15.0847  Lc=7.6652  Ls=0.7420\n",
      "[123400/160000] loss=12.1506  Lc=5.9681  Ls=0.6182\n",
      "[123600/160000] loss=12.8269  Lc=6.5148  Ls=0.6312\n",
      "[123800/160000] loss=11.7816  Lc=5.5234  Ls=0.6258\n",
      "[124000/160000] loss=12.8624  Lc=6.4059  Ls=0.6457\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_124000.pth\n",
      "[124200/160000] loss=11.8951  Lc=5.1603  Ls=0.6735\n",
      "[124400/160000] loss=13.0132  Lc=6.5243  Ls=0.6489\n",
      "[124600/160000] loss=18.8606  Lc=8.3163  Ls=1.0544\n",
      "[124800/160000] loss=13.0243  Lc=6.6532  Ls=0.6371\n",
      "[125000/160000] loss=15.1612  Lc=6.9152  Ls=0.8246\n",
      "[125200/160000] loss=10.6558  Lc=5.6820  Ls=0.4974\n",
      "[125400/160000] loss=15.3568  Lc=7.3619  Ls=0.7995\n",
      "[125600/160000] loss=13.5430  Lc=6.8453  Ls=0.6698\n",
      "[125800/160000] loss=11.2771  Lc=5.6631  Ls=0.5614\n",
      "[126000/160000] loss=17.0268  Lc=8.3482  Ls=0.8679\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_126000.pth\n",
      "[126200/160000] loss=27.7039  Lc=11.7213  Ls=1.5983\n",
      "[126400/160000] loss=13.0854  Lc=6.4878  Ls=0.6598\n",
      "[126600/160000] loss=10.4114  Lc=5.2447  Ls=0.5167\n",
      "[126800/160000] loss=14.2614  Lc=6.7569  Ls=0.7504\n",
      "[127000/160000] loss=12.6020  Lc=6.0064  Ls=0.6596\n",
      "[127200/160000] loss=14.4101  Lc=6.8035  Ls=0.7607\n",
      "[127400/160000] loss=13.8896  Lc=6.6923  Ls=0.7197\n",
      "[127600/160000] loss=20.9600  Lc=9.5400  Ls=1.1420\n",
      "[127800/160000] loss=21.2262  Lc=11.1524  Ls=1.0074\n",
      "[128000/160000] loss=16.4086  Lc=8.2842  Ls=0.8124\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_128000.pth\n",
      "[128200/160000] loss=22.8391  Lc=10.8336  Ls=1.2005\n",
      "[128400/160000] loss=8.9883  Lc=4.1138  Ls=0.4874\n",
      "[128600/160000] loss=14.6422  Lc=7.5489  Ls=0.7093\n",
      "[128800/160000] loss=14.6146  Lc=6.1684  Ls=0.8446\n",
      "[129000/160000] loss=11.9737  Lc=5.8318  Ls=0.6142\n",
      "[129200/160000] loss=19.3491  Lc=8.9832  Ls=1.0366\n",
      "[129400/160000] loss=17.8280  Lc=9.1510  Ls=0.8677\n",
      "[129600/160000] loss=13.3182  Lc=5.6920  Ls=0.7626\n",
      "[129800/160000] loss=16.1754  Lc=8.4393  Ls=0.7736\n",
      "[130000/160000] loss=13.2600  Lc=6.7286  Ls=0.6531\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_130000.pth\n",
      "[130200/160000] loss=18.0268  Lc=7.4418  Ls=1.0585\n",
      "[130400/160000] loss=23.8489  Lc=10.2442  Ls=1.3605\n",
      "[130600/160000] loss=10.1852  Lc=4.9709  Ls=0.5214\n",
      "[130800/160000] loss=13.9749  Lc=7.3364  Ls=0.6639\n",
      "[131000/160000] loss=10.3249  Lc=4.9688  Ls=0.5356\n",
      "[131200/160000] loss=13.1016  Lc=6.3199  Ls=0.6782\n",
      "[131400/160000] loss=17.2662  Lc=7.7275  Ls=0.9539\n",
      "[131600/160000] loss=14.8098  Lc=6.7421  Ls=0.8068\n",
      "[131800/160000] loss=12.6422  Lc=6.1672  Ls=0.6475\n",
      "[132000/160000] loss=13.1574  Lc=6.5654  Ls=0.6592\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_132000.pth\n",
      "[132200/160000] loss=16.5115  Lc=8.0257  Ls=0.8486\n",
      "[132400/160000] loss=13.9078  Lc=6.3773  Ls=0.7530\n",
      "[132600/160000] loss=18.8378  Lc=7.2895  Ls=1.1548\n",
      "[132800/160000] loss=14.5294  Lc=6.8380  Ls=0.7691\n",
      "[133000/160000] loss=10.6878  Lc=5.5650  Ls=0.5123\n",
      "[133200/160000] loss=9.8043  Lc=4.8730  Ls=0.4931\n",
      "[133400/160000] loss=14.0871  Lc=6.7553  Ls=0.7332\n",
      "[133600/160000] loss=10.1871  Lc=5.8909  Ls=0.4296\n",
      "[133800/160000] loss=20.7711  Lc=10.7126  Ls=1.0058\n",
      "[134000/160000] loss=13.3580  Lc=6.1964  Ls=0.7162\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_134000.pth\n",
      "[134200/160000] loss=13.0445  Lc=6.8110  Ls=0.6233\n",
      "[134400/160000] loss=10.1302  Lc=5.3222  Ls=0.4808\n",
      "[134600/160000] loss=11.0632  Lc=5.5201  Ls=0.5543\n",
      "[134800/160000] loss=14.3071  Lc=7.0197  Ls=0.7287\n",
      "[135000/160000] loss=13.6988  Lc=7.2405  Ls=0.6458\n",
      "[135200/160000] loss=9.6153  Lc=4.8158  Ls=0.4799\n",
      "[135400/160000] loss=14.6463  Lc=7.6856  Ls=0.6961\n",
      "[135600/160000] loss=16.5411  Lc=8.3341  Ls=0.8207\n",
      "[135800/160000] loss=15.2624  Lc=7.4545  Ls=0.7808\n",
      "[136000/160000] loss=10.3093  Lc=5.1231  Ls=0.5186\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_136000.pth\n",
      "[136200/160000] loss=10.5383  Lc=5.6763  Ls=0.4862\n",
      "[136400/160000] loss=8.8095  Lc=4.2540  Ls=0.4555\n",
      "[136600/160000] loss=13.6104  Lc=6.4548  Ls=0.7156\n",
      "[136800/160000] loss=17.2833  Lc=9.0662  Ls=0.8217\n",
      "[137000/160000] loss=17.3012  Lc=8.6361  Ls=0.8665\n",
      "[137200/160000] loss=20.7353  Lc=8.9573  Ls=1.1778\n",
      "[137400/160000] loss=9.1929  Lc=4.4510  Ls=0.4742\n",
      "[137600/160000] loss=13.0364  Lc=6.5768  Ls=0.6460\n",
      "[137800/160000] loss=12.1976  Lc=5.7444  Ls=0.6453\n",
      "[138000/160000] loss=16.7477  Lc=8.3686  Ls=0.8379\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_138000.pth\n",
      "[138200/160000] loss=17.1155  Lc=7.4444  Ls=0.9671\n",
      "[138400/160000] loss=11.3343  Lc=5.9106  Ls=0.5424\n",
      "[138600/160000] loss=12.3860  Lc=6.0837  Ls=0.6302\n",
      "[138800/160000] loss=15.4866  Lc=7.6069  Ls=0.7880\n",
      "[139000/160000] loss=12.4397  Lc=6.3699  Ls=0.6070\n",
      "[139200/160000] loss=10.8517  Lc=5.4627  Ls=0.5389\n",
      "[139400/160000] loss=11.4586  Lc=5.3444  Ls=0.6114\n",
      "[139600/160000] loss=12.6412  Lc=6.3475  Ls=0.6294\n",
      "[139800/160000] loss=17.8635  Lc=8.3911  Ls=0.9472\n",
      "[140000/160000] loss=16.6167  Lc=8.1666  Ls=0.8450\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_140000.pth\n",
      "[140200/160000] loss=19.3654  Lc=7.8044  Ls=1.1561\n",
      "[140400/160000] loss=8.5778  Lc=4.2487  Ls=0.4329\n",
      "[140600/160000] loss=13.5243  Lc=6.1133  Ls=0.7411\n",
      "[140800/160000] loss=15.3644  Lc=8.3376  Ls=0.7027\n",
      "[141000/160000] loss=15.1664  Lc=7.9292  Ls=0.7237\n",
      "[141200/160000] loss=11.1513  Lc=6.0241  Ls=0.5127\n",
      "[141400/160000] loss=13.2081  Lc=6.0114  Ls=0.7197\n",
      "[141600/160000] loss=15.2121  Lc=7.2252  Ls=0.7987\n",
      "[141800/160000] loss=12.1890  Lc=6.1099  Ls=0.6079\n",
      "[142000/160000] loss=10.8980  Lc=5.3625  Ls=0.5536\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_142000.pth\n",
      "[142200/160000] loss=9.2533  Lc=4.4421  Ls=0.4811\n",
      "[142400/160000] loss=14.7434  Lc=6.3485  Ls=0.8395\n",
      "[142600/160000] loss=12.5913  Lc=6.3399  Ls=0.6251\n",
      "[142800/160000] loss=12.5739  Lc=5.9419  Ls=0.6632\n",
      "[143000/160000] loss=11.2229  Lc=5.7681  Ls=0.5455\n",
      "[143200/160000] loss=16.0219  Lc=7.7516  Ls=0.8270\n",
      "[143400/160000] loss=10.3775  Lc=5.3403  Ls=0.5037\n",
      "[143600/160000] loss=10.5726  Lc=5.3778  Ls=0.5195\n",
      "[143800/160000] loss=11.2493  Lc=5.5256  Ls=0.5724\n",
      "[144000/160000] loss=20.9455  Lc=10.5804  Ls=1.0365\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_144000.pth\n",
      "[144200/160000] loss=10.5117  Lc=5.6993  Ls=0.4812\n",
      "[144400/160000] loss=10.5256  Lc=5.0889  Ls=0.5437\n",
      "[144600/160000] loss=18.8027  Lc=9.7803  Ls=0.9022\n",
      "[144800/160000] loss=13.6958  Lc=6.6825  Ls=0.7013\n",
      "[145000/160000] loss=12.3557  Lc=6.5635  Ls=0.5792\n",
      "[145200/160000] loss=13.8019  Lc=6.4234  Ls=0.7379\n",
      "[145400/160000] loss=8.9466  Lc=4.3958  Ls=0.4551\n",
      "[145600/160000] loss=14.3257  Lc=6.9652  Ls=0.7361\n",
      "[145800/160000] loss=13.6816  Lc=7.2751  Ls=0.6406\n",
      "[146000/160000] loss=18.7808  Lc=8.7011  Ls=1.0080\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_146000.pth\n",
      "[146200/160000] loss=12.8543  Lc=5.8335  Ls=0.7021\n",
      "[146400/160000] loss=13.8306  Lc=7.2776  Ls=0.6553\n",
      "[146600/160000] loss=14.3869  Lc=7.8886  Ls=0.6498\n",
      "[146800/160000] loss=10.8002  Lc=5.6203  Ls=0.5180\n",
      "[147000/160000] loss=9.7651  Lc=5.0340  Ls=0.4731\n",
      "[147200/160000] loss=12.6633  Lc=6.8769  Ls=0.5786\n",
      "[147400/160000] loss=23.6083  Lc=11.7354  Ls=1.1873\n",
      "[147600/160000] loss=18.9743  Lc=9.4657  Ls=0.9509\n",
      "[147800/160000] loss=13.3644  Lc=6.5880  Ls=0.6776\n",
      "[148000/160000] loss=13.3730  Lc=5.6110  Ls=0.7762\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_148000.pth\n",
      "[148200/160000] loss=17.6368  Lc=8.5468  Ls=0.9090\n",
      "[148400/160000] loss=16.6255  Lc=8.2974  Ls=0.8328\n",
      "[148600/160000] loss=9.2851  Lc=4.3427  Ls=0.4942\n",
      "[148800/160000] loss=15.2435  Lc=7.8343  Ls=0.7409\n",
      "[149000/160000] loss=9.3625  Lc=4.0696  Ls=0.5293\n",
      "[149200/160000] loss=14.8936  Lc=7.1233  Ls=0.7770\n",
      "[149400/160000] loss=17.1001  Lc=8.3072  Ls=0.8793\n",
      "[149600/160000] loss=10.7600  Lc=5.4486  Ls=0.5311\n",
      "[149800/160000] loss=12.0798  Lc=5.8126  Ls=0.6267\n",
      "[150000/160000] loss=14.2825  Lc=7.7404  Ls=0.6542\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_150000.pth\n",
      "[150200/160000] loss=11.1249  Lc=5.1725  Ls=0.5952\n",
      "[150400/160000] loss=14.0138  Lc=7.3775  Ls=0.6636\n",
      "[150600/160000] loss=13.2514  Lc=6.3909  Ls=0.6861\n",
      "[150800/160000] loss=15.6363  Lc=7.2873  Ls=0.8349\n",
      "[151000/160000] loss=12.2121  Lc=6.0516  Ls=0.6161\n",
      "[151200/160000] loss=14.5944  Lc=6.5943  Ls=0.8000\n",
      "[151400/160000] loss=19.3667  Lc=7.0437  Ls=1.2323\n",
      "[151600/160000] loss=10.2493  Lc=5.5696  Ls=0.4680\n",
      "[151800/160000] loss=28.2832  Lc=9.6316  Ls=1.8652\n",
      "[152000/160000] loss=12.8409  Lc=6.6954  Ls=0.6145\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_152000.pth\n",
      "[152200/160000] loss=15.0315  Lc=7.3115  Ls=0.7720\n",
      "[152400/160000] loss=18.3484  Lc=8.2571  Ls=1.0091\n",
      "[152600/160000] loss=12.1637  Lc=6.4118  Ls=0.5752\n",
      "[152800/160000] loss=14.1435  Lc=7.1256  Ls=0.7018\n",
      "[153000/160000] loss=15.0731  Lc=7.2142  Ls=0.7859\n",
      "[153200/160000] loss=10.5858  Lc=4.9066  Ls=0.5679\n",
      "[153400/160000] loss=13.4820  Lc=7.4184  Ls=0.6064\n",
      "[153600/160000] loss=10.3164  Lc=5.3440  Ls=0.4972\n",
      "[153800/160000] loss=10.4607  Lc=5.4935  Ls=0.4967\n",
      "[154000/160000] loss=16.1988  Lc=7.9622  Ls=0.8237\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_154000.pth\n",
      "[154200/160000] loss=19.7452  Lc=9.5142  Ls=1.0231\n",
      "[154400/160000] loss=19.3486  Lc=10.1500  Ls=0.9199\n",
      "[154600/160000] loss=15.2072  Lc=7.5984  Ls=0.7609\n",
      "[154800/160000] loss=12.8192  Lc=6.1930  Ls=0.6626\n",
      "[155000/160000] loss=16.7861  Lc=8.3814  Ls=0.8405\n",
      "[155200/160000] loss=10.1322  Lc=4.7284  Ls=0.5404\n",
      "[155400/160000] loss=10.8889  Lc=5.6009  Ls=0.5288\n",
      "[155600/160000] loss=16.2425  Lc=8.1387  Ls=0.8104\n",
      "[155800/160000] loss=14.1699  Lc=6.8967  Ls=0.7273\n",
      "[156000/160000] loss=9.9927  Lc=4.8738  Ls=0.5119\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_156000.pth\n",
      "[156200/160000] loss=10.0889  Lc=5.3979  Ls=0.4691\n",
      "[156400/160000] loss=13.1847  Lc=6.6665  Ls=0.6518\n",
      "[156600/160000] loss=14.5604  Lc=6.9967  Ls=0.7564\n",
      "[156800/160000] loss=11.7685  Lc=6.1769  Ls=0.5592\n",
      "[157000/160000] loss=14.9182  Lc=7.1342  Ls=0.7784\n",
      "[157200/160000] loss=16.0372  Lc=8.6342  Ls=0.7403\n",
      "[157400/160000] loss=12.4145  Lc=6.3927  Ls=0.6022\n",
      "[157600/160000] loss=13.7370  Lc=6.7545  Ls=0.6983\n",
      "[157800/160000] loss=11.2715  Lc=5.6860  Ls=0.5585\n",
      "[158000/160000] loss=10.7544  Lc=5.5079  Ls=0.5247\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_158000.pth\n",
      "[158200/160000] loss=14.4548  Lc=7.5908  Ls=0.6864\n",
      "[158400/160000] loss=12.1658  Lc=5.9941  Ls=0.6172\n",
      "[158600/160000] loss=10.0044  Lc=4.8009  Ls=0.5203\n",
      "[158800/160000] loss=13.4767  Lc=6.2916  Ls=0.7185\n",
      "[159000/160000] loss=17.4070  Lc=8.3076  Ls=0.9099\n",
      "[159200/160000] loss=13.0094  Lc=6.4603  Ls=0.6549\n",
      "[159400/160000] loss=11.4484  Lc=5.5174  Ls=0.5931\n",
      "[159600/160000] loss=14.9987  Lc=7.4728  Ls=0.7526\n",
      "[159800/160000] loss=14.1638  Lc=7.6352  Ls=0.6529\n",
      "[160000/160000] loss=12.0419  Lc=5.5896  Ls=0.6452\n",
      "[ckpt] saved: ./adain_runs/decoder_iter_160000.pth\n",
      "[final] saved: ./adain_runs/decoder_final.pth\n"
     ]
    }
   ],
   "source": [
    "# --- cell 4: training (fixed) ---\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def denorm_for_save(x):\n",
    "    # x is normalized (ImageNet), bring it back to [0,1] for saving\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=x.device).view(1,3,1,1)\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225], device=x.device).view(1,3,1,1)\n",
    "    y = x * std + mean\n",
    "    return torch.clamp(y, 0, 1)\n",
    "\n",
    "def save_checkpoint(iteration, decoder, opt, cfg, extra_samples=None):\n",
    "    ckpt = {\n",
    "        'iteration': iteration,\n",
    "        'decoder': decoder.state_dict(),\n",
    "        'optimizer': opt.state_dict(),\n",
    "        'cfg': cfg.__dict__,\n",
    "        'time': time.time(),\n",
    "    }\n",
    "    path = os.path.join(cfg.out_dir, f\"decoder_iter_{iteration}.pth\")\n",
    "    torch.save(ckpt, path)\n",
    "    print(f\"[ckpt] saved: {path}\")\n",
    "\n",
    "    # save a grid of sample images if provided (for quick visual check)\n",
    "    if extra_samples is not None:\n",
    "        grid = utils.make_grid(extra_samples, nrow=len(extra_samples)//2 or 4)\n",
    "        utils.save_image(grid, os.path.join(cfg.out_dir, f\"samples_iter_{iteration}.png\"))\n",
    "\n",
    "def load_checkpoint(path, decoder, opt=None):\n",
    "    data = torch.load(path, map_location='cpu')\n",
    "    decoder.load_state_dict(data['decoder'], strict=True)\n",
    "    if opt is not None and 'optimizer' in data:\n",
    "        opt.load_state_dict(data['optimizer'])\n",
    "    print(f\"[ckpt] loaded: {path} @ iter={data.get('iteration')}\")\n",
    "    return data.get('iteration', 0)\n",
    "\n",
    "# Instantiate nets\n",
    "decoder = Decoder().to(device)\n",
    "lossnet = LossNet().to(device).eval()  # VGG features (frozen)\n",
    "\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=cfg.lr)\n",
    "\n",
    "start_iter = 0\n",
    "if cfg.resume:\n",
    "    start_iter = load_checkpoint(cfg.resume, decoder, optimizer)\n",
    "\n",
    "# Training\n",
    "decoder.train()\n",
    "global_iter = start_iter\n",
    "\n",
    "# Simple infinite loader over epochs until max_iterations\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "while global_iter < cfg.max_iterations:\n",
    "    try:\n",
    "        content, style = next(data_iter)\n",
    "    except StopIteration:\n",
    "        data_iter = iter(train_loader)\n",
    "        content, style = next(data_iter)\n",
    "\n",
    "    content = content.to(device, non_blocking=True)\n",
    "    style   = style.to(device, non_blocking=True)\n",
    "\n",
    "    # --- Encode targets (constants for this step) ---\n",
    "    # Use no_grad ONLY for target encodes; keep graph for generated encodes.\n",
    "    with torch.no_grad():\n",
    "        c4 = lossnet.encoder(content, out_keys=['relu4_1'])['relu4_1']\n",
    "        s4 = lossnet.encoder(style,   out_keys=['relu4_1'])['relu4_1']\n",
    "\n",
    "    # AdaIN target feature (no grad needed; serves as fixed target)\n",
    "    t_feat = adain(c4, s4)\n",
    "\n",
    "    # --- Decode: we need gradients through the decoder ---\n",
    "    g_img = decoder(t_feat)                      # (B,3,H,W) in \"normalized\" space\n",
    "    g_img_clamped = torch.clamp(g_img, -3.0, 3.0)  # keep stability; retains grad\n",
    "\n",
    "    # --- Re-encode generated for losses (WITH grad) ---\n",
    "    g_feats_all = lossnet.encoder(g_img_clamped, out_keys=STYLE_LAYERS)\n",
    "\n",
    "    # Style features for targets (no grad)\n",
    "    with torch.no_grad():\n",
    "        s_feats_all = lossnet.encoder(style, out_keys=STYLE_LAYERS)\n",
    "\n",
    "    # --- Losses ---\n",
    "    # Content loss: || f(g(t))_relu4_1 - t ||^2\n",
    "    loss_c = F.mse_loss(g_feats_all[CONTENT_LAYER], t_feat)\n",
    "    # Style loss: mean/std over multiple layers\n",
    "    loss_s = mean_std_loss(g_feats_all, s_feats_all)\n",
    "    loss   = loss_c + cfg.lambda_style * loss_s\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    global_iter += 1\n",
    "\n",
    "    # Optional tiny grad sanity after first update\n",
    "    if global_iter == start_iter + 1:\n",
    "        gmean = sum((p.grad.abs().mean().item() for p in decoder.parameters() if p.grad is not None))\n",
    "        print(\"Grad sanity (decoder mean |grad|):\", f\"{gmean:.6f}\")\n",
    "\n",
    "    if global_iter % cfg.log_every == 0:\n",
    "        print(f\"[{global_iter:>6d}/{cfg.max_iterations}] \"\n",
    "              f\"loss={loss.item():.4f}  Lc={loss_c.item():.4f}  Ls={loss_s.item():.4f}\")\n",
    "\n",
    "    if global_iter % cfg.save_every == 0:\n",
    "        # Save checkpoint + a quick sample grid (content, style, stylized)\n",
    "        with torch.no_grad():\n",
    "            c_show = denorm_for_save(content[:2])\n",
    "            s_show = denorm_for_save(style[:2])\n",
    "            g_show = denorm_for_save(g_img_clamped[:2])\n",
    "            samples = torch.cat([c_show, s_show, g_show], dim=0)\n",
    "        save_checkpoint(global_iter, decoder, optimizer, cfg, extra_samples=samples)\n",
    "\n",
    "# Final save\n",
    "final_path = os.path.join(cfg.out_dir, \"decoder_final.pth\")\n",
    "torch.save({'iteration': global_iter, 'decoder': decoder.state_dict(), 'cfg': cfg.__dict__}, final_path)\n",
    "print(f\"[final] saved: {final_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780b80f0-dfa5-4184-a50c-f5f8f70cfef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Preferred checkpoint not found: /Test_image/adain_runs/decoder_iter_160000.pth\n",
      "Using checkpoint: /workspace/adain_runs/decoder_iter_160000.pth\n",
      "[stylize] saved: /workspace/Test_image/adain_runs/test_stylized.png\n"
     ]
    }
   ],
   "source": [
    "# --- Self-contained AdaIN inference cell (defines everything + runs test) ---\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, utils\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# ------------------------ CONFIG (edit these) ------------------------\n",
    "content_path_in = \"content test2.jpg\"   # your content image\n",
    "style_path_in   = \"Van_Gogh_-_Starry_Night.jpg\"     # your style image\n",
    "preferred_ckpt  = \"/Test_image/adain_runs/decoder_iter_160000.pth\"  # try this first\n",
    "fallback_dirs   = [\"adain_runs\", \"Test_image/adain_runs\"]          # auto-search if needed\n",
    "out_path        = \"Test_image/adain_runs/test_stylized3.png\"\n",
    "alpha           = 1.0      # 0..1 (0 more content, 1 more style)\n",
    "max_side        = 512      # resize shorter side to this\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# --- helpers: normalization / AdaIN ---\n",
    "def calc_mean_std(feat, eps=1e-5):\n",
    "    B, C = feat.size()[:2]\n",
    "    feat_var = feat.view(B, C, -1).var(dim=2, unbiased=False) + eps\n",
    "    feat_std = feat_var.sqrt().view(B, C, 1, 1)\n",
    "    feat_mean = feat.view(B, C, -1).mean(dim=2).view(B, C, 1, 1)\n",
    "    return feat_mean, feat_std\n",
    "\n",
    "def adain(content_feat, style_feat, eps=1e-5):\n",
    "    c_mean, c_std = calc_mean_std(content_feat, eps)\n",
    "    s_mean, s_std = calc_mean_std(style_feat, eps)\n",
    "    normalized = (content_feat - c_mean) / c_std\n",
    "    return normalized * s_std + s_mean\n",
    "\n",
    "@torch.no_grad()\n",
    "def denorm_for_save(x):\n",
    "    # x assumed normalized for ImageNet\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=x.device).view(1,3,1,1)\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225], device=x.device).view(1,3,1,1)\n",
    "    y = x * std + mean\n",
    "    return torch.clamp(y, 0, 1)\n",
    "\n",
    "# --- VGG19 encoder up to relu4_1 (frozen), returns selected layers ---\n",
    "LAYER_NAME_MAP = { 1:'relu1_1', 6:'relu2_1', 11:'relu3_1', 20:'relu4_1' }\n",
    "STYLE_LAYERS = ['relu1_1','relu2_1','relu3_1','relu4_1']\n",
    "CONTENT_LAYER = 'relu4_1'\n",
    "\n",
    "class VGGEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features\n",
    "        for p in self.vgg.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x, out_keys=('relu4_1',)):\n",
    "        feats = {}\n",
    "        h = x\n",
    "        for i, layer in enumerate(self.vgg):\n",
    "            h = layer(h)\n",
    "            name = LAYER_NAME_MAP.get(i, None)\n",
    "            if name in out_keys:\n",
    "                feats[name] = h\n",
    "            if i >= 20:\n",
    "                # we only care up to relu4_1\n",
    "                pass\n",
    "        return feats\n",
    "\n",
    "# --- Decoder (must match what you trained) ---\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(512, 256, 3), nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(256, 256, 3), nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(256, 256, 3), nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(256, 256, 3), nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(256, 128, 3), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(128, 128, 3), nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(128, 64, 3), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(64, 64, 3), nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(64, 3, 3)  # last conv, no activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "# --- image IO ---\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "def load_image_for_vgg(path: Path, max_side=512):\n",
    "    im = Image.open(path).convert('RGB')\n",
    "    w, h = im.size\n",
    "    if min(w, h) != max_side:\n",
    "        if w < h:\n",
    "            new_w, new_h = max_side, int(h * max_side / w)\n",
    "        else:\n",
    "            new_h, new_w = max_side, int(w * max_side / h)\n",
    "        im = im.resize((new_w, new_h), Image.BICUBIC)\n",
    "    x = T.ToTensor()(im)\n",
    "    x = T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)(x)\n",
    "    return x.unsqueeze(0)\n",
    "\n",
    "# --- path resolution & checkpoint finder ---\n",
    "CWD = Path.cwd()\n",
    "def resolve(p: str|Path) -> Path:\n",
    "    p = Path(p)\n",
    "    return p if p.is_absolute() else (CWD / p)\n",
    "\n",
    "def find_latest_ckpt(search_dirs):\n",
    "    best = None\n",
    "    for d in search_dirs:\n",
    "        d = resolve(d)\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        for p in d.glob(\"decoder_iter_*.pth\"):\n",
    "            try:\n",
    "                it = int(p.stem.split(\"_\")[-1])\n",
    "                best = max(best, (it, p)) if best else (it, p)\n",
    "            except:\n",
    "                pass\n",
    "        if best is None:\n",
    "            fin = d / \"decoder_final.pth\"\n",
    "            if fin.is_file():\n",
    "                best = (0, fin)\n",
    "    return best[1] if best else None\n",
    "\n",
    "content_path = resolve(content_path_in)\n",
    "style_path   = resolve(style_path_in)\n",
    "if not content_path.is_file():\n",
    "    raise FileNotFoundError(f\"Content image not found: {content_path}\")\n",
    "if not style_path.is_file():\n",
    "    raise FileNotFoundError(f\"Style image not found: {style_path}\")\n",
    "\n",
    "ckpt_path = resolve(preferred_ckpt)\n",
    "if not ckpt_path.is_file():\n",
    "    print(f\"Preferred checkpoint not found: {ckpt_path}\")\n",
    "    ckpt_path = find_latest_ckpt([preferred_ckpt] + fallback_dirs)\n",
    "    if ckpt_path is None:\n",
    "        print(\"Top-level:\", list(CWD.iterdir()))\n",
    "        if (CWD/\"adain_runs\").exists():\n",
    "            print(\"adain_runs:\", list((CWD/\"adain_runs\").glob(\"*\")))\n",
    "        if (CWD/\"Test_image\").exists():\n",
    "            print(\"Test_image:\", list((CWD/\"Test_image\").glob(\"*\")))\n",
    "        raise FileNotFoundError(\"No checkpoint found in the usual locations.\")\n",
    "print(\"Using checkpoint:\", ckpt_path)\n",
    "\n",
    "# --- run stylization ---\n",
    "@torch.no_grad()\n",
    "def stylize_one(content_path: Path, style_path: Path, decoder_ckpt: Path,\n",
    "                alpha=1.0, out_path=\"stylized.png\", max_side=512):\n",
    "    # nets\n",
    "    dec = Decoder().to(device).eval()\n",
    "    ckpt = torch.load(str(decoder_ckpt), map_location=device)\n",
    "    dec.load_state_dict(ckpt['decoder'])\n",
    "\n",
    "    enc = VGGEncoder().to(device).eval()\n",
    "\n",
    "    # imgs\n",
    "    c = load_image_for_vgg(content_path, max_side=max_side).to(device)\n",
    "    s = load_image_for_vgg(style_path,   max_side=max_side).to(device)\n",
    "\n",
    "    # AdaIN at relu4_1\n",
    "    c4 = enc(c, out_keys=['relu4_1'])['relu4_1']\n",
    "    s4 = enc(s, out_keys=['relu4_1'])['relu4_1']\n",
    "    t  = adain(c4, s4)\n",
    "    if alpha < 1.0:\n",
    "        t = alpha * t + (1 - alpha) * c4\n",
    "\n",
    "    y = dec(t).clamp(-3, 3)\n",
    "    y_img = denorm_for_save(y)\n",
    "\n",
    "    out_path = resolve(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    utils.save_image(y_img, str(out_path))\n",
    "    print(f\"[stylize] saved: {out_path}\")\n",
    "\n",
    "stylize_one(\n",
    "    content_path=content_path,\n",
    "    style_path=style_path,\n",
    "    decoder_ckpt=ckpt_path,\n",
    "    alpha=alpha,\n",
    "    out_path=out_path,\n",
    "    max_side=max_side\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd0411-caca-4789-8768-9e1580a72128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
