# -*- coding: utf-8 -*-
"""Final Project mid report.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18IQBOaGI6tE6Tt9wlVsq18OEKtGomFi6
"""

from google.colab import files
uploaded = files.upload()

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.applications import vgg19
from tensorflow.keras.preprocessing import image
import tensorflow.keras.backend as K

# --- 2. Helper Functions ---
def load_and_process_img(path_to_img, max_dim=400):
    # Always load as RGB
    img = image.load_img(path_to_img, target_size=(max_dim, max_dim), color_mode='rgb')
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = vgg19.preprocess_input(img)
    return img
    """
    Loads an image from disk and preprocesses it for VGG19.
    - Resizes to a square (max_dim x max_dim).
    - Converts to array and adds batch dimension.
    - Applies VGG19 preprocessing (BGR order, mean subtraction).
    Returns: float32 tensor of shape (1, H, W, 3).
    """
    # Always load as RGB (VGG expects 3 channels)

def deprocess_img(processed_img):
    """
    Inverse of VGG19 preprocessing.
    - Removes batch dimension if present.
    - Adds back ImageNet means.
    - Converts BGR -> RGB.
    - Clips to valid [0, 255] and returns uint8 for display.
    """
    x = processed_img.copy()
    if len(x.shape) == 4:
        x = np.squeeze(x, 0)
            # Add back mean values that were subtracted by preprocess_input
    x[:, :, 0] += 103.939
    x[:, :, 1] += 116.779
    x[:, :, 2] += 123.68
    x = x[:, :, ::-1]
    x = np.clip(x, 0, 255).astype('uint8')
    return x
def show_img(img, title=None):
    plt.imshow(img)
    if title:
        plt.title(title)
    plt.axis('off')
    plt.show()

# --- 3. Load Images ---
# Paths to content and style images (update these if your files live elsewhere)
content_path = '/content/content_test2.jpg'
style_path = '/content/Van_Gogh_-_Starry_Night.jpg'
# Preprocessed (VGG-ready) tensors
content_img = load_and_process_img(content_path)
style_img = load_and_process_img(style_path)
# Quick visual sanity check (convert back to display space)
show_img(deprocess_img(content_img), "Content Image")
show_img(deprocess_img(style_img), "Style Image")

# --- 4. Define Content and Style Layers ---
# Content usually from deeper layers (captures structure)
content_layers = ['block5_conv2']
# Style usually aggregated from multiple layers (captures textures/patterns at different scales)
style_layers = [
    'block1_conv1',
    'block2_conv1',
    'block3_conv1',
    'block4_conv1',
    'block5_conv1'
]
num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

# --- 5. Build the Model ---
def get_model():
    """
    Loads VGG19 (imagenet weights, no top) and returns a model that outputs
    the activations for the specified style and content layers in a single forward pass.
    """

    vgg = vgg19.VGG19(include_top=False, weights='imagenet')
    vgg.trainable = False
    outputs = [vgg.get_layer(name).output for name in (style_layers + content_layers)]
    model = tf.keras.Model([vgg.input], outputs)
    return model

# --- 6. Loss Functions ---
def gram_matrix(input_tensor):
    """
    Computes the (normalized) Gram matrix for a batch of feature maps.
    - Measures correlations between channels (style representation).
    input:  tensor of shape (B, H, W, C)
    return: tensor of shape (B, C, C)
    """
    # Einstein summation: multiply features with themselves across spatial dims

    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
    input_shape = tf.shape(input_tensor)
    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
    return result / num_locations

def get_feature_representations(model, content_img, style_img):
    """
    Runs both images through the model once and splits outputs into style/content lists.
    Returns:
      - style_features: list of style layer activations for style image
      - content_features: list of content layer activations for content image
    """
    style_outputs = model(style_img)
    content_outputs = model(content_img)
        # First N outputs are style layers; last M outputs are content layers
    style_features = [style_layer for style_layer in style_outputs[:num_style_layers]]
    content_features = [content_layer for content_layer in content_outputs[num_style_layers:]]
    return style_features, content_features

def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):
    """
    Computes the total loss (style + content + total variation) for the current generated image.
    - model: VGG wrapper that outputs selected layers
    - loss_weights: tuple (style_w, content_w, tv_w)
    - init_image: tf.Variable being optimized (the generated image)
    - gram_style_features: target style Gram matrices from the style image
    - content_features: target content activations from the content image
    Returns tuple: (total_loss, style_loss, content_loss, total_variation_loss)
    """

    style_weight, content_weight, total_variation_weight = loss_weights
        # Forward pass on the current generated image
    model_outputs = model(init_image)
    style_output_features = model_outputs[:num_style_layers]
    content_output_features = model_outputs[num_style_layers:]

    style_score = 0
    content_score = 0

    # --- Style loss: average across style layers ---
    weight_per_style_layer = 1.0 / float(num_style_layers)
    for target_style, comb_style in zip(gram_style_features, style_output_features):
        comb_gram = gram_matrix(comb_style)
        style_score += weight_per_style_layer * tf.reduce_mean(tf.square(comb_gram - target_style))

    # --- Content loss: average across content layers (often only one) ---
    weight_per_content_layer = 1.0 / float(num_content_layers)
    for target_content, comb_content in zip(content_features, content_output_features):
        content_score += weight_per_content_layer * tf.reduce_mean(tf.square(comb_content - target_content))

    # --- Total variation loss: encourages spatial smoothness in the generated image ---
    total_variation_score = tf.image.total_variation(init_image)

    # Weighted sum of the three components
    loss = (style_weight * style_score) + (content_weight * content_score) + (total_variation_weight * total_variation_score)

    return loss, style_score, content_score, total_variation_score

# --- 7. Optimization Loop ---
@tf.function()
def compute_grads(cfg):
    """
    Computes gradients of total loss w.r.t. the generated image.
    - Wrapped in tf.function for speed (graph mode).
    Returns: (grads, (total_loss, style_loss, content_loss, tv_loss))
    """
    with tf.GradientTape() as tape:
        all_loss = compute_loss(**cfg)
    total_loss = all_loss[0]
    return tape.gradient(total_loss, cfg['init_image']), all_loss

# --- 8. Run Style Transfer ---
import time

# Weights for each loss component
style_weight = 1e2
content_weight = 1e4
total_variation_weight = 8
# Build feature-extractor model and freeze it
model = get_model()
for layer in model.layers:
    layer.trainable = False

# Extract target features once (no need each iteration)
style_features, content_features = get_feature_representations(model, content_img, style_img)
gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]

# Initialize generated image
init_image = tf.Variable(content_img, dtype=tf.float32)

# Optimizer for the image variable (high LR commonly used for NST with clipping)
opt = tf.optimizers.Adam(learning_rate=5.0)

# Pack config for convenience
loss_weights = (style_weight, content_weight, total_variation_weight)
cfg = {
    'model': model,
    'loss_weights': loss_weights,
    'init_image': init_image,
    'gram_style_features': gram_style_features,
    'content_features': content_features
}
# Training schedule
epochs = 11
steps_per_epoch = 40

best_loss, best_img = float('inf'), None

for n in range(epochs):
    for m in range(steps_per_epoch):
      # Compute gradients and individual loss components
        grads, all_loss = compute_grads(cfg)
        loss, style_score, content_score, total_variation_score = all_loss
                # Apply one optimization step to the image
        opt.apply_gradients([(grads, init_image)])
        # Clip to keep pixel values in a reasonable range in VGG space
        # (preprocessed images are roughly centered around 0 with ~[-128, 128] spread)
        clipped = tf.clip_by_value(init_image, -128.0, 128.0)
        init_image.assign(clipped)
        if loss < best_loss:
            best_loss = loss
            best_img = deprocess_img(init_image.numpy())
        # Log progress and show intermediate result every 30 iterations
        iteration = n * steps_per_epoch + m + 1
        # Track the best (lowest) total loss image so far
        if iteration % 30 == 0:
            print(
                f"Iteration {iteration}, "
                f"Loss: {float(loss):.2f}, "
                f"Style: {float(style_score):.2f}, "
                f"Content: {float(content_score):.2f}, "
                f"TV: {float(total_variation_score):.2f}"
            )
            show_img(deprocess_img(init_image.numpy()), f"Output at iteration {iteration}")

print("Optimization complete.")
show_img(best_img, "Best Stylized Image")

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.applications import vgg19
from tensorflow.keras.preprocessing import image
import tensorflow.keras.backend as K

# --- 2. Helper Functions ---
def load_and_process_img(path_to_img, max_dim=400):
    # Always load as RGB
    img = image.load_img(path_to_img, target_size=(max_dim, max_dim), color_mode='rgb')
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = vgg19.preprocess_input(img)
    return img
    """
    Loads an image from disk and preprocesses it for VGG19.
    - Resizes to a square (max_dim x max_dim).
    - Converts to array and adds batch dimension.
    - Applies VGG19 preprocessing (BGR order, mean subtraction).
    Returns: float32 tensor of shape (1, H, W, 3).
    """
    # Always load as RGB (VGG expects 3 channels)

def deprocess_img(processed_img):
    """
    Inverse of VGG19 preprocessing.
    - Removes batch dimension if present.
    - Adds back ImageNet means.
    - Converts BGR -> RGB.
    - Clips to valid [0, 255] and returns uint8 for display.
    """
    x = processed_img.copy()
    if len(x.shape) == 4:
        x = np.squeeze(x, 0)
            # Add back mean values that were subtracted by preprocess_input
    x[:, :, 0] += 103.939
    x[:, :, 1] += 116.779
    x[:, :, 2] += 123.68
    x = x[:, :, ::-1]
    x = np.clip(x, 0, 255).astype('uint8')
    return x
def show_img(img, title=None):
    plt.imshow(img)
    if title:
        plt.title(title)
    plt.axis('off')
    plt.show()

# --- 3. Load Images ---
# Paths to content and style images (update these if your files live elsewhere)
content_path = '/content/content_test2.jpg'
style_path = '/content/Van_Gogh_-_Starry_Night.jpg'
# Preprocessed (VGG-ready) tensors
content_img = load_and_process_img(content_path)
style_img = load_and_process_img(style_path)
# Quick visual sanity check (convert back to display space)
show_img(deprocess_img(content_img), "Content Image")
show_img(deprocess_img(style_img), "Style Image")

# --- 4. Define Content and Style Layers ---
# Content usually from deeper layers (captures structure)
content_layers = ['block5_conv2']
# Style usually aggregated from multiple layers (captures textures/patterns at different scales)
style_layers = [
    'block1_conv1',
    'block2_conv1',
    'block3_conv1',
    'block4_conv1',
    'block5_conv1'
]
num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

# --- 5. Build the Model ---
def get_model():
    """
    Loads VGG19 (imagenet weights, no top) and returns a model that outputs
    the activations for the specified style and content layers in a single forward pass.
    """

    vgg = vgg19.VGG19(include_top=False, weights='imagenet')
    vgg.trainable = False
    outputs = [vgg.get_layer(name).output for name in (style_layers + content_layers)]
    model = tf.keras.Model([vgg.input], outputs)
    return model

# --- 6. Loss Functions ---
def gram_matrix(input_tensor):
    """
    Computes the (normalized) Gram matrix for a batch of feature maps.
    - Measures correlations between channels (style representation).
    input:  tensor of shape (B, H, W, C)
    return: tensor of shape (B, C, C)
    """
    # Einstein summation: multiply features with themselves across spatial dims

    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
    input_shape = tf.shape(input_tensor)
    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
    return result / num_locations

def get_feature_representations(model, content_img, style_img):
    """
    Runs both images through the model once and splits outputs into style/content lists.
    Returns:
      - style_features: list of style layer activations for style image
      - content_features: list of content layer activations for content image
    """
    style_outputs = model(style_img)
    content_outputs = model(content_img)
        # First N outputs are style layers; last M outputs are content layers
    style_features = [style_layer for style_layer in style_outputs[:num_style_layers]]
    content_features = [content_layer for content_layer in content_outputs[num_style_layers:]]
    return style_features, content_features

def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):
    """
    Computes the total loss (style + content + total variation) for the current generated image.
    - model: VGG wrapper that outputs selected layers
    - loss_weights: tuple (style_w, content_w, tv_w)
    - init_image: tf.Variable being optimized (the generated image)
    - gram_style_features: target style Gram matrices from the style image
    - content_features: target content activations from the content image
    Returns tuple: (total_loss, style_loss, content_loss, total_variation_loss)
    """

    style_weight, content_weight, total_variation_weight = loss_weights
        # Forward pass on the current generated image
    model_outputs = model(init_image)
    style_output_features = model_outputs[:num_style_layers]
    content_output_features = model_outputs[num_style_layers:]

    style_score = 0
    content_score = 0

    # --- Style loss: average across style layers ---
    weight_per_style_layer = 1.0 / float(num_style_layers)
    for target_style, comb_style in zip(gram_style_features, style_output_features):
        comb_gram = gram_matrix(comb_style)
        style_score += weight_per_style_layer * tf.reduce_mean(tf.square(comb_gram - target_style))

    # --- Content loss: average across content layers (often only one) ---
    weight_per_content_layer = 1.0 / float(num_content_layers)
    for target_content, comb_content in zip(content_features, content_output_features):
        content_score += weight_per_content_layer * tf.reduce_mean(tf.square(comb_content - target_content))

    # --- Total variation loss: encourages spatial smoothness in the generated image ---
    total_variation_score = tf.image.total_variation(init_image)

    # Weighted sum of the three components
    loss = (style_weight * style_score) + (content_weight * content_score) + (total_variation_weight * total_variation_score)

    return loss, style_score, content_score, total_variation_score

# --- 7. Optimization Loop ---
@tf.function()
def compute_grads(cfg):
    """
    Computes gradients of total loss w.r.t. the generated image.
    - Wrapped in tf.function for speed (graph mode).
    Returns: (grads, (total_loss, style_loss, content_loss, tv_loss))
    """
    with tf.GradientTape() as tape:
        all_loss = compute_loss(**cfg)
    total_loss = all_loss[0]
    return tape.gradient(total_loss, cfg['init_image']), all_loss

# --- 8. Run Style Transfer ---
import time

# Weights for each loss component
style_weight = 3e3
content_weight = 1e3
total_variation_weight = 1e-6
# Build feature-extractor model and freeze it
model = get_model()
for layer in model.layers:
    layer.trainable = False

# Extract target features once (no need each iteration)
style_features, content_features = get_feature_representations(model, content_img, style_img)
gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]

# Initialize generated image
init_image = tf.Variable(content_img, dtype=tf.float32)

# Optimizer for the image variable (high LR commonly used for NST with clipping)
opt = tf.optimizers.Adam(learning_rate=5.0)

# Pack config for convenience
loss_weights = (style_weight, content_weight, total_variation_weight)
cfg = {
    'model': model,
    'loss_weights': loss_weights,
    'init_image': init_image,
    'gram_style_features': gram_style_features,
    'content_features': content_features
}
# Training schedule
epochs = 11
steps_per_epoch = 40

best_loss, best_img = float('inf'), None

for n in range(epochs):
    for m in range(steps_per_epoch):
      # Compute gradients and individual loss components
        grads, all_loss = compute_grads(cfg)
        loss, style_score, content_score, total_variation_score = all_loss
                # Apply one optimization step to the image
        opt.apply_gradients([(grads, init_image)])
        # Clip to keep pixel values in a reasonable range in VGG space
        # (preprocessed images are roughly centered around 0 with ~[-128, 128] spread)
        clipped = tf.clip_by_value(init_image, -128.0, 128.0)
        init_image.assign(clipped)
        if loss < best_loss:
            best_loss = loss
            best_img = deprocess_img(init_image.numpy())
        # Log progress and show intermediate result every 30 iterations
        iteration = n * steps_per_epoch + m + 1
        # Track the best (lowest) total loss image so far
        if iteration % 30 == 0:
            print(
                f"Iteration {iteration}, "
                f"Loss: {float(loss):.2f}, "
                f"Style: {float(style_score):.2f}, "
                f"Content: {float(content_score):.2f}, "
                f"TV: {float(total_variation_score):.2f}"
            )
            show_img(deprocess_img(init_image.numpy()), f"Output at iteration {iteration}")

print("Optimization complete.")
show_img(best_img, "Best Stylized Image")

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.applications import vgg19
from tensorflow.keras.preprocessing import image
import tensorflow.keras.backend as K

# --- 2. Helper Functions ---
def load_and_process_img(path_to_img, max_dim=400):
    # Always load as RGB
    img = image.load_img(path_to_img, target_size=(max_dim, max_dim), color_mode='rgb')
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = vgg19.preprocess_input(img)
    return img
    """
    Loads an image from disk and preprocesses it for VGG19.
    - Resizes to a square (max_dim x max_dim).
    - Converts to array and adds batch dimension.
    - Applies VGG19 preprocessing (BGR order, mean subtraction).
    Returns: float32 tensor of shape (1, H, W, 3).
    """
    # Always load as RGB (VGG expects 3 channels)

def deprocess_img(processed_img):
    """
    Inverse of VGG19 preprocessing.
    - Removes batch dimension if present.
    - Adds back ImageNet means.
    - Converts BGR -> RGB.
    - Clips to valid [0, 255] and returns uint8 for display.
    """
    x = processed_img.copy()
    if len(x.shape) == 4:
        x = np.squeeze(x, 0)
            # Add back mean values that were subtracted by preprocess_input
    x[:, :, 0] += 103.939
    x[:, :, 1] += 116.779
    x[:, :, 2] += 123.68
    x = x[:, :, ::-1]
    x = np.clip(x, 0, 255).astype('uint8')
    return x
def show_img(img, title=None):
    plt.imshow(img)
    if title:
        plt.title(title)
    plt.axis('off')
    plt.show()

# --- 3. Load Images ---
# Paths to content and style images (update these if your files live elsewhere)
content_path = '/content/content_test2.jpg'
style_path = '/content/Van_Gogh_-_Starry_Night.jpg'
# Preprocessed (VGG-ready) tensors
content_img = load_and_process_img(content_path)
style_img = load_and_process_img(style_path)
# Quick visual sanity check (convert back to display space)
show_img(deprocess_img(content_img), "Content Image")
show_img(deprocess_img(style_img), "Style Image")

# --- 4. Define Content and Style Layers ---
# Content usually from deeper layers (captures structure)
content_layers = ['block5_conv2']
# Style usually aggregated from multiple layers (captures textures/patterns at different scales)
style_layers = [
    'block1_conv1',
    'block2_conv1',
    'block3_conv1',
    'block4_conv1',
    'block5_conv1'
]
num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

# --- 5. Build the Model ---
def get_model():
    """
    Loads VGG19 (imagenet weights, no top) and returns a model that outputs
    the activations for the specified style and content layers in a single forward pass.
    """

    vgg = vgg19.VGG19(include_top=False, weights='imagenet')
    vgg.trainable = False
    outputs = [vgg.get_layer(name).output for name in (style_layers + content_layers)]
    model = tf.keras.Model([vgg.input], outputs)
    return model

# --- 6. Loss Functions ---
def gram_matrix(input_tensor):
    """
    Computes the (normalized) Gram matrix for a batch of feature maps.
    - Measures correlations between channels (style representation).
    input:  tensor of shape (B, H, W, C)
    return: tensor of shape (B, C, C)
    """
    # Einstein summation: multiply features with themselves across spatial dims

    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
    input_shape = tf.shape(input_tensor)
    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
    return result / num_locations

def get_feature_representations(model, content_img, style_img):
    """
    Runs both images through the model once and splits outputs into style/content lists.
    Returns:
      - style_features: list of style layer activations for style image
      - content_features: list of content layer activations for content image
    """
    style_outputs = model(style_img)
    content_outputs = model(content_img)
        # First N outputs are style layers; last M outputs are content layers
    style_features = [style_layer for style_layer in style_outputs[:num_style_layers]]
    content_features = [content_layer for content_layer in content_outputs[num_style_layers:]]
    return style_features, content_features

def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):
    """
    Computes the total loss (style + content + total variation) for the current generated image.
    - model: VGG wrapper that outputs selected layers
    - loss_weights: tuple (style_w, content_w, tv_w)
    - init_image: tf.Variable being optimized (the generated image)
    - gram_style_features: target style Gram matrices from the style image
    - content_features: target content activations from the content image
    Returns tuple: (total_loss, style_loss, content_loss, total_variation_loss)
    """

    style_weight, content_weight, total_variation_weight = loss_weights
        # Forward pass on the current generated image
    model_outputs = model(init_image)
    style_output_features = model_outputs[:num_style_layers]
    content_output_features = model_outputs[num_style_layers:]

    style_score = 0
    content_score = 0

    # --- Style loss: average across style layers ---
    weight_per_style_layer = 1.0 / float(num_style_layers)
    for target_style, comb_style in zip(gram_style_features, style_output_features):
        comb_gram = gram_matrix(comb_style)
        style_score += weight_per_style_layer * tf.reduce_mean(tf.square(comb_gram - target_style))

    # --- Content loss: average across content layers (often only one) ---
    weight_per_content_layer = 1.0 / float(num_content_layers)
    for target_content, comb_content in zip(content_features, content_output_features):
        content_score += weight_per_content_layer * tf.reduce_mean(tf.square(comb_content - target_content))

    # --- Total variation loss: encourages spatial smoothness in the generated image ---
    total_variation_score = tf.image.total_variation(init_image)

    # Weighted sum of the three components
    loss = (style_weight * style_score) + (content_weight * content_score) + (total_variation_weight * total_variation_score)

    return loss, style_score, content_score, total_variation_score

# --- 7. Optimization Loop ---
@tf.function()
def compute_grads(cfg):
    """
    Computes gradients of total loss w.r.t. the generated image.
    - Wrapped in tf.function for speed (graph mode).
    Returns: (grads, (total_loss, style_loss, content_loss, tv_loss))
    """
    with tf.GradientTape() as tape:
        all_loss = compute_loss(**cfg)
    total_loss = all_loss[0]
    return tape.gradient(total_loss, cfg['init_image']), all_loss

# --- 8. Run Style Transfer ---
import time

# Weights for each loss component
style_weight = 1e3
content_weight = 3e3
total_variation_weight = 1e-6
# Build feature-extractor model and freeze it
model = get_model()
for layer in model.layers:
    layer.trainable = False

# Extract target features once (no need each iteration)
style_features, content_features = get_feature_representations(model, content_img, style_img)
gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]

# Initialize generated image
init_image = tf.Variable(content_img, dtype=tf.float32)

# Optimizer for the image variable (high LR commonly used for NST with clipping)
opt = tf.optimizers.Adam(learning_rate=5.0)

# Pack config for convenience
loss_weights = (style_weight, content_weight, total_variation_weight)
cfg = {
    'model': model,
    'loss_weights': loss_weights,
    'init_image': init_image,
    'gram_style_features': gram_style_features,
    'content_features': content_features
}
# Training schedule
epochs = 11
steps_per_epoch = 40

best_loss, best_img = float('inf'), None

for n in range(epochs):
    for m in range(steps_per_epoch):
      # Compute gradients and individual loss components
        grads, all_loss = compute_grads(cfg)
        loss, style_score, content_score, total_variation_score = all_loss
                # Apply one optimization step to the image
        opt.apply_gradients([(grads, init_image)])
        # Clip to keep pixel values in a reasonable range in VGG space
        # (preprocessed images are roughly centered around 0 with ~[-128, 128] spread)
        clipped = tf.clip_by_value(init_image, -128.0, 128.0)
        init_image.assign(clipped)
        if loss < best_loss:
            best_loss = loss
            best_img = deprocess_img(init_image.numpy())
        # Log progress and show intermediate result every 30 iterations
        iteration = n * steps_per_epoch + m + 1
        # Track the best (lowest) total loss image so far
        if iteration % 30 == 0:
            print(
                f"Iteration {iteration}, "
                f"Loss: {float(loss):.2f}, "
                f"Style: {float(style_score):.2f}, "
                f"Content: {float(content_score):.2f}, "
                f"TV: {float(total_variation_score):.2f}"
            )
            show_img(deprocess_img(init_image.numpy()), f"Output at iteration {iteration}")

print("Optimization complete.")
show_img(best_img, "Best Stylized Image")

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.applications import vgg19
from tensorflow.keras.preprocessing import image
import tensorflow.keras.backend as K

# --- 2. Helper Functions ---
def load_and_process_img(path_to_img, max_dim=400):
    # Always load as RGB
    img = image.load_img(path_to_img, target_size=(max_dim, max_dim), color_mode='rgb')
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = vgg19.preprocess_input(img)
    return img
    """
    Loads an image from disk and preprocesses it for VGG19.
    - Resizes to a square (max_dim x max_dim).
    - Converts to array and adds batch dimension.
    - Applies VGG19 preprocessing (BGR order, mean subtraction).
    Returns: float32 tensor of shape (1, H, W, 3).
    """
    # Always load as RGB (VGG expects 3 channels)

def deprocess_img(processed_img):
    """
    Inverse of VGG19 preprocessing.
    - Removes batch dimension if present.
    - Adds back ImageNet means.
    - Converts BGR -> RGB.
    - Clips to valid [0, 255] and returns uint8 for display.
    """
    x = processed_img.copy()
    if len(x.shape) == 4:
        x = np.squeeze(x, 0)
            # Add back mean values that were subtracted by preprocess_input
    x[:, :, 0] += 103.939
    x[:, :, 1] += 116.779
    x[:, :, 2] += 123.68
    x = x[:, :, ::-1]
    x = np.clip(x, 0, 255).astype('uint8')
    return x
def show_img(img, title=None):
    plt.imshow(img)
    if title:
        plt.title(title)
    plt.axis('off')
    plt.show()

# --- 3. Load Images ---
# Paths to content and style images (update these if your files live elsewhere)
content_path = '/content/content_test2.jpg'
style_path = '/content/Van_Gogh_-_Starry_Night.jpg'
# Preprocessed (VGG-ready) tensors
content_img = load_and_process_img(content_path)
style_img = load_and_process_img(style_path)
# Quick visual sanity check (convert back to display space)
show_img(deprocess_img(content_img), "Content Image")
show_img(deprocess_img(style_img), "Style Image")

# --- 4. Define Content and Style Layers ---
# Content usually from deeper layers (captures structure)
content_layers = ['block5_conv2']
# Style usually aggregated from multiple layers (captures textures/patterns at different scales)
style_layers = [
    'block1_conv1',
    'block2_conv1',
    'block3_conv1',
    'block4_conv1',
    'block5_conv1'
]
num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

# --- 5. Build the Model ---
def get_model():
    """
    Loads VGG19 (imagenet weights, no top) and returns a model that outputs
    the activations for the specified style and content layers in a single forward pass.
    """

    vgg = vgg19.VGG19(include_top=False, weights='imagenet')
    vgg.trainable = False
    outputs = [vgg.get_layer(name).output for name in (style_layers + content_layers)]
    model = tf.keras.Model([vgg.input], outputs)
    return model

# --- 6. Loss Functions ---
def gram_matrix(input_tensor):
    """
    Computes the (normalized) Gram matrix for a batch of feature maps.
    - Measures correlations between channels (style representation).
    input:  tensor of shape (B, H, W, C)
    return: tensor of shape (B, C, C)
    """
    # Einstein summation: multiply features with themselves across spatial dims

    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
    input_shape = tf.shape(input_tensor)
    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
    return result / num_locations

def get_feature_representations(model, content_img, style_img):
    """
    Runs both images through the model once and splits outputs into style/content lists.
    Returns:
      - style_features: list of style layer activations for style image
      - content_features: list of content layer activations for content image
    """
    style_outputs = model(style_img)
    content_outputs = model(content_img)
        # First N outputs are style layers; last M outputs are content layers
    style_features = [style_layer for style_layer in style_outputs[:num_style_layers]]
    content_features = [content_layer for content_layer in content_outputs[num_style_layers:]]
    return style_features, content_features

def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):
    """
    Computes the total loss (style + content + total variation) for the current generated image.
    - model: VGG wrapper that outputs selected layers
    - loss_weights: tuple (style_w, content_w, tv_w)
    - init_image: tf.Variable being optimized (the generated image)
    - gram_style_features: target style Gram matrices from the style image
    - content_features: target content activations from the content image
    Returns tuple: (total_loss, style_loss, content_loss, total_variation_loss)
    """

    style_weight, content_weight, total_variation_weight = loss_weights
        # Forward pass on the current generated image
    model_outputs = model(init_image)
    style_output_features = model_outputs[:num_style_layers]
    content_output_features = model_outputs[num_style_layers:]

    style_score = 0
    content_score = 0

    # --- Style loss: average across style layers ---
    weight_per_style_layer = 1.0 / float(num_style_layers)
    for target_style, comb_style in zip(gram_style_features, style_output_features):
        comb_gram = gram_matrix(comb_style)
        style_score += weight_per_style_layer * tf.reduce_mean(tf.square(comb_gram - target_style))

    # --- Content loss: average across content layers (often only one) ---
    weight_per_content_layer = 1.0 / float(num_content_layers)
    for target_content, comb_content in zip(content_features, content_output_features):
        content_score += weight_per_content_layer * tf.reduce_mean(tf.square(comb_content - target_content))

    # --- Total variation loss: encourages spatial smoothness in the generated image ---
    total_variation_score = tf.image.total_variation(init_image)

    # Weighted sum of the three components
    loss = (style_weight * style_score) + (content_weight * content_score) + (total_variation_weight * total_variation_score)

    return loss, style_score, content_score, total_variation_score

# --- 7. Optimization Loop ---
@tf.function()
def compute_grads(cfg):
    """
    Computes gradients of total loss w.r.t. the generated image.
    - Wrapped in tf.function for speed (graph mode).
    Returns: (grads, (total_loss, style_loss, content_loss, tv_loss))
    """
    with tf.GradientTape() as tape:
        all_loss = compute_loss(**cfg)
    total_loss = all_loss[0]
    return tape.gradient(total_loss, cfg['init_image']), all_loss

# --- 8. Run Style Transfer ---
import time

# Weights for each loss component
style_weight = 4e3
content_weight = 2e5
total_variation_weight = 1e-6
# Build feature-extractor model and freeze it
model = get_model()
for layer in model.layers:
    layer.trainable = False

# Extract target features once (no need each iteration)
style_features, content_features = get_feature_representations(model, content_img, style_img)
gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]

# Initialize generated image
init_image = tf.Variable(content_img, dtype=tf.float32)

# Optimizer for the image variable (high LR commonly used for NST with clipping)
opt = tf.optimizers.Adam(learning_rate=5.0)

# Pack config for convenience
loss_weights = (style_weight, content_weight, total_variation_weight)
cfg = {
    'model': model,
    'loss_weights': loss_weights,
    'init_image': init_image,
    'gram_style_features': gram_style_features,
    'content_features': content_features
}
# Training schedule
epochs = 11
steps_per_epoch = 40

best_loss, best_img = float('inf'), None

for n in range(epochs):
    for m in range(steps_per_epoch):
      # Compute gradients and individual loss components
        grads, all_loss = compute_grads(cfg)
        loss, style_score, content_score, total_variation_score = all_loss
                # Apply one optimization step to the image
        opt.apply_gradients([(grads, init_image)])
        # Clip to keep pixel values in a reasonable range in VGG space
        # (preprocessed images are roughly centered around 0 with ~[-128, 128] spread)
        clipped = tf.clip_by_value(init_image, -128.0, 128.0)
        init_image.assign(clipped)
        if loss < best_loss:
            best_loss = loss
            best_img = deprocess_img(init_image.numpy())
        # Log progress and show intermediate result every 30 iterations
        iteration = n * steps_per_epoch + m + 1
        # Track the best (lowest) total loss image so far
        if iteration % 30 == 0:
            print(
                f"Iteration {iteration}, "
                f"Loss: {float(loss):.2f}, "
                f"Style: {float(style_score):.2f}, "
                f"Content: {float(content_score):.2f}, "
                f"TV: {float(total_variation_score):.2f}"
            )
            show_img(deprocess_img(init_image.numpy()), f"Output at iteration {iteration}")

print("Optimization complete.")
show_img(best_img, "Best Stylized Image")

# --- Save final outputs ---
from pathlib import Path
from datetime import datetime
from PIL import Image

# Ensure we have a proper uint8 image for saving
final_img = deprocess_img(init_image.numpy())   # image at the end of training
best_img  = best_img if best_img is not None else final_img  # fallback safety

ts = datetime.now().strftime("%Y%m%d_%H%M%S")

# Build descriptive filenames
style_stem   = Path(style_path).stem.replace(" ", "_")
content_stem = Path(content_path).stem.replace(" ", "_")
base = f"{content_stem}__{style_stem}__sw{style_weight:g}_cw{content_weight:g}_tv{total_variation_weight:g}_{ts}"

best_path  = f"{base}__BEST.png"
final_path = f"{base}__FINAL.png"

# Save PNGs
Image.fromarray(best_img).save(best_path)
Image.fromarray(final_img).save(final_path)

print(f"Saved:\n  {best_path}\n  {final_path}")